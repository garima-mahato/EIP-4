{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "b2c4233e-55ea-49d8-d40b-4985fa3b7342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/Assignment5/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2wvwvVICP1Y",
        "colab_type": "code",
        "outputId": "99a524bb-348e-4b4c-9ede-49301e1f4679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!ls 'gdrive/My Drive/Assignment5/hvc_data.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'gdrive/My Drive/Assignment5/hvc_data.zip'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "1cdaa6a8-0620-41c3-bbd2-7aebd9b34781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16, ResNet50\n",
        "from keras.layers.core import Dropout\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9B5LoH3DVhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"hvc_annotations.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnBEOnalDY9r",
        "colab_type": "code",
        "outputId": "af9dbac4-d29d-4587-b862-3f100721fd46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>images/Set1/5580_2 (3).jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>images/Set1/4650_1 (4).jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>images/Set1/44880_0.jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>images/Set1/26130_2.jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>images/Set1/IMG (4438).jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13568</th>\n",
              "      <td>images/Set3/32880_0.jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Happy</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13570.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13569</th>\n",
              "      <td>images/Set4/12210_0.jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>25-35</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Fancy</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13571.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13570</th>\n",
              "      <td>images/Set4/110190_3.jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Bad</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Side</td>\n",
              "      <td>resized/13572.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13571</th>\n",
              "      <td>images/Set4/4830_0.jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Bad</td>\n",
              "      <td>25-35</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13573.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13572</th>\n",
              "      <td>images/Set4/5040_1.jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>25-35</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Happy</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13574.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13573 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         filename  gender  ...        bodypose         image_path\n",
              "0      images/Set1/5580_2 (3).jpg    male  ...  Front-Frontish      resized/1.jpg\n",
              "1      images/Set1/4650_1 (4).jpg  female  ...  Front-Frontish      resized/2.jpg\n",
              "2         images/Set1/44880_0.jpg    male  ...  Front-Frontish      resized/3.jpg\n",
              "3         images/Set1/26130_2.jpg    male  ...  Front-Frontish      resized/4.jpg\n",
              "4      images/Set1/IMG (4438).jpg  female  ...  Front-Frontish      resized/5.jpg\n",
              "...                           ...     ...  ...             ...                ...\n",
              "13568     images/Set3/32880_0.jpg    male  ...  Front-Frontish  resized/13570.jpg\n",
              "13569     images/Set4/12210_0.jpg  female  ...  Front-Frontish  resized/13571.jpg\n",
              "13570    images/Set4/110190_3.jpg  female  ...            Side  resized/13572.jpg\n",
              "13571      images/Set4/4830_0.jpg  female  ...  Front-Frontish  resized/13573.jpg\n",
              "13572      images/Set4/5040_1.jpg    male  ...  Front-Frontish  resized/13574.jpg\n",
              "\n",
              "[13573 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "c4345a92-1bf9-4201-80d5-99eeab51cb13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "bf32613e-6169-4e04-d85e-8bc654fb9bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "0c6011db-9063-4af9-b01d-8969b9b06338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "b843a85c-699e-4d49-afa5-c86f8a3d69f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3890</th>\n",
              "      <td>resized/3891.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11098</th>\n",
              "      <td>resized/11100.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3391</th>\n",
              "      <td>resized/3392.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2407</th>\n",
              "      <td>resized/2408.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6378</th>\n",
              "      <td>resized/6379.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "3890    resized/3891.jpg              0  ...                        0              1\n",
              "11098  resized/11100.jpg              1  ...                        0              1\n",
              "3391    resized/3392.jpg              1  ...                        1              0\n",
              "2407    resized/2408.jpg              1  ...                        1              0\n",
              "6378    resized/6379.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bh4gQLqSe-M",
        "colab_type": "code",
        "outputId": "97e5a890-15f6-4c7b-b7fb-4106031d3d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "val_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10565</th>\n",
              "      <td>resized/10567.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9077</th>\n",
              "      <td>resized/9078.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11463</th>\n",
              "      <td>resized/11465.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12304</th>\n",
              "      <td>resized/12306.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2521</th>\n",
              "      <td>resized/2522.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "10565  resized/10567.jpg              0  ...                        1              0\n",
              "9077    resized/9078.jpg              0  ...                        1              0\n",
              "11463  resized/11465.jpg              1  ...                        0              1\n",
              "12304  resized/12306.jpg              0  ...                        1              0\n",
              "2521    resized/2522.jpg              1  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "c4a25745-ae30-4fe9-a246-7dd70882bb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "73ebfcdd-a80d-46aa-cc12-cb373cfe508a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "backbone = ResNet50(\n",
        "    weights=\"imagenet\", \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "#neck = Conv2D(512,1)(neck)\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    #neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(256, activation=\"relu\")(in_layer)\n",
        "    #neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "outputId": "9b7c9613-17a6-4811-8730-ff071c96c889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "11dc31fd-aafd-4ff4-e883-297bb5bb7b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/10\n",
            "360/360 [==============================] - 58s 162ms/step - loss: 7.4087 - gender_output_loss: 0.5150 - image_quality_output_loss: 0.9721 - age_output_loss: 1.4680 - weight_output_loss: 1.0189 - bag_output_loss: 0.9192 - footwear_output_loss: 0.8816 - pose_output_loss: 0.6915 - emotion_output_loss: 0.9426 - gender_output_acc: 0.7493 - image_quality_output_acc: 0.5342 - age_output_acc: 0.3676 - weight_output_acc: 0.6201 - bag_output_acc: 0.5829 - footwear_output_acc: 0.6080 - pose_output_acc: 0.7055 - emotion_output_acc: 0.7025 - val_loss: 7.1591 - val_gender_output_loss: 0.4639 - val_image_quality_output_loss: 0.9593 - val_age_output_loss: 1.3957 - val_weight_output_loss: 0.9871 - val_bag_output_loss: 0.8892 - val_footwear_output_loss: 0.8316 - val_pose_output_loss: 0.7370 - val_emotion_output_loss: 0.8953 - val_gender_output_acc: 0.7807 - val_image_quality_output_acc: 0.5524 - val_age_output_acc: 0.4153 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.7036 - val_emotion_output_acc: 0.7132\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "360/360 [==============================] - 45s 124ms/step - loss: 6.3219 - gender_output_loss: 0.3487 - image_quality_output_loss: 0.8634 - age_output_loss: 1.3397 - weight_output_loss: 0.9226 - bag_output_loss: 0.8046 - footwear_output_loss: 0.7350 - pose_output_loss: 0.4550 - emotion_output_loss: 0.8529 - gender_output_acc: 0.8470 - image_quality_output_acc: 0.5917 - age_output_acc: 0.4122 - weight_output_acc: 0.6411 - bag_output_acc: 0.6483 - footwear_output_acc: 0.6784 - pose_output_acc: 0.8176 - emotion_output_acc: 0.7088 - val_loss: 7.4400 - val_gender_output_loss: 0.7206 - val_image_quality_output_loss: 0.9537 - val_age_output_loss: 1.3786 - val_weight_output_loss: 0.9714 - val_bag_output_loss: 0.9138 - val_footwear_output_loss: 0.8207 - val_pose_output_loss: 0.7030 - val_emotion_output_loss: 0.9781 - val_gender_output_acc: 0.6996 - val_image_quality_output_acc: 0.5499 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5912 - val_footwear_output_acc: 0.6401 - val_pose_output_acc: 0.7203 - val_emotion_output_acc: 0.6578\n",
            "Epoch 3/10\n",
            "360/360 [==============================] - 45s 124ms/step - loss: 5.4758 - gender_output_loss: 0.2440 - image_quality_output_loss: 0.7753 - age_output_loss: 1.2300 - weight_output_loss: 0.8188 - bag_output_loss: 0.6854 - footwear_output_loss: 0.6406 - pose_output_loss: 0.3038 - emotion_output_loss: 0.7779 - gender_output_acc: 0.8991 - image_quality_output_acc: 0.6398 - age_output_acc: 0.4676 - weight_output_acc: 0.6714 - bag_output_acc: 0.7093 - footwear_output_acc: 0.7170 - pose_output_acc: 0.8816 - emotion_output_acc: 0.7217 - val_loss: 7.7040 - val_gender_output_loss: 0.6272 - val_image_quality_output_loss: 0.9533 - val_age_output_loss: 1.4089 - val_weight_output_loss: 1.0531 - val_bag_output_loss: 1.0065 - val_footwear_output_loss: 0.8550 - val_pose_output_loss: 0.8746 - val_emotion_output_loss: 0.9254 - val_gender_output_acc: 0.7399 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.5852 - val_footwear_output_acc: 0.6386 - val_pose_output_acc: 0.7072 - val_emotion_output_acc: 0.7001\n",
            "Epoch 4/10\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.7046 - gender_output_loss: 0.1752 - image_quality_output_loss: 0.6787 - age_output_loss: 1.1071 - weight_output_loss: 0.7052 - bag_output_loss: 0.5849 - footwear_output_loss: 0.5414 - pose_output_loss: 0.2033 - emotion_output_loss: 0.7088 - gender_output_acc: 0.9335 - image_quality_output_acc: 0.6899 - age_output_acc: 0.5247 - weight_output_acc: 0.7046 - bag_output_acc: 0.7580 - footwear_output_acc: 0.7638 - pose_output_acc: 0.9224 - emotion_output_acc: 0.7397\n",
            "360/360 [==============================] - 45s 124ms/step - loss: 4.7051 - gender_output_loss: 0.1756 - image_quality_output_loss: 0.6789 - age_output_loss: 1.1063 - weight_output_loss: 0.7052 - bag_output_loss: 0.5848 - footwear_output_loss: 0.5416 - pose_output_loss: 0.2030 - emotion_output_loss: 0.7095 - gender_output_acc: 0.9334 - image_quality_output_acc: 0.6898 - age_output_acc: 0.5251 - weight_output_acc: 0.7043 - bag_output_acc: 0.7580 - footwear_output_acc: 0.7636 - pose_output_acc: 0.9224 - emotion_output_acc: 0.7395 - val_loss: 8.1831 - val_gender_output_loss: 0.9439 - val_image_quality_output_loss: 0.9937 - val_age_output_loss: 1.4366 - val_weight_output_loss: 1.0909 - val_bag_output_loss: 1.0282 - val_footwear_output_loss: 0.8467 - val_pose_output_loss: 0.7942 - val_emotion_output_loss: 1.0490 - val_gender_output_acc: 0.7006 - val_image_quality_output_acc: 0.5282 - val_age_output_acc: 0.3861 - val_weight_output_acc: 0.6300 - val_bag_output_acc: 0.5670 - val_footwear_output_acc: 0.6341 - val_pose_output_acc: 0.7359 - val_emotion_output_acc: 0.5963\n",
            "Epoch 5/10\n",
            "360/360 [==============================] - 45s 125ms/step - loss: 3.9324 - gender_output_loss: 0.1271 - image_quality_output_loss: 0.5729 - age_output_loss: 0.9605 - weight_output_loss: 0.5889 - bag_output_loss: 0.4565 - footwear_output_loss: 0.4565 - pose_output_loss: 0.1490 - emotion_output_loss: 0.6211 - gender_output_acc: 0.9528 - image_quality_output_acc: 0.7455 - age_output_acc: 0.5972 - weight_output_acc: 0.7613 - bag_output_acc: 0.8171 - footwear_output_acc: 0.8043 - pose_output_acc: 0.9459 - emotion_output_acc: 0.7679 - val_loss: 9.3664 - val_gender_output_loss: 0.6209 - val_image_quality_output_loss: 1.0503 - val_age_output_loss: 1.8203 - val_weight_output_loss: 1.6735 - val_bag_output_loss: 1.1610 - val_footwear_output_loss: 0.9248 - val_pose_output_loss: 0.9858 - val_emotion_output_loss: 1.1295 - val_gender_output_acc: 0.7727 - val_image_quality_output_acc: 0.5333 - val_age_output_acc: 0.2772 - val_weight_output_acc: 0.4183 - val_bag_output_acc: 0.4627 - val_footwear_output_acc: 0.6346 - val_pose_output_acc: 0.7122 - val_emotion_output_acc: 0.5418\n",
            "Epoch 6/10\n",
            "360/360 [==============================] - 45s 124ms/step - loss: 3.2529 - gender_output_loss: 0.1090 - image_quality_output_loss: 0.4708 - age_output_loss: 0.8124 - weight_output_loss: 0.4724 - bag_output_loss: 0.3513 - footwear_output_loss: 0.3798 - pose_output_loss: 0.1280 - emotion_output_loss: 0.5291 - gender_output_acc: 0.9583 - image_quality_output_acc: 0.7989 - age_output_acc: 0.6648 - weight_output_acc: 0.8095 - bag_output_acc: 0.8594 - footwear_output_acc: 0.8392 - pose_output_acc: 0.9529 - emotion_output_acc: 0.8030 - val_loss: 10.2328 - val_gender_output_loss: 1.3716 - val_image_quality_output_loss: 1.1182 - val_age_output_loss: 1.6371 - val_weight_output_loss: 1.2587 - val_bag_output_loss: 1.4447 - val_footwear_output_loss: 1.0455 - val_pose_output_loss: 0.9760 - val_emotion_output_loss: 1.3810 - val_gender_output_acc: 0.6517 - val_image_quality_output_acc: 0.5222 - val_age_output_acc: 0.3649 - val_weight_output_acc: 0.5806 - val_bag_output_acc: 0.5126 - val_footwear_output_acc: 0.6043 - val_pose_output_acc: 0.7132 - val_emotion_output_acc: 0.4753\n",
            "Epoch 7/10\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.5692 - gender_output_loss: 0.0997 - image_quality_output_loss: 0.3609 - age_output_loss: 0.6596 - weight_output_loss: 0.3643 - bag_output_loss: 0.2622 - footwear_output_loss: 0.3104 - pose_output_loss: 0.0974 - emotion_output_loss: 0.4148 - gender_output_acc: 0.9617 - image_quality_output_acc: 0.8490 - age_output_acc: 0.7371 - weight_output_acc: 0.8542 - bag_output_acc: 0.8995 - footwear_output_acc: 0.8755 - pose_output_acc: 0.9646 - emotion_output_acc: 0.8482\n",
            "360/360 [==============================] - 45s 125ms/step - loss: 2.5705 - gender_output_loss: 0.0999 - image_quality_output_loss: 0.3613 - age_output_loss: 0.6600 - weight_output_loss: 0.3644 - bag_output_loss: 0.2623 - footwear_output_loss: 0.3103 - pose_output_loss: 0.0974 - emotion_output_loss: 0.4150 - gender_output_acc: 0.9615 - image_quality_output_acc: 0.8486 - age_output_acc: 0.7370 - weight_output_acc: 0.8542 - bag_output_acc: 0.8994 - footwear_output_acc: 0.8754 - pose_output_acc: 0.9646 - emotion_output_acc: 0.8480 - val_loss: 10.6837 - val_gender_output_loss: 1.3030 - val_image_quality_output_loss: 1.4496 - val_age_output_loss: 1.6973 - val_weight_output_loss: 1.2990 - val_bag_output_loss: 1.3017 - val_footwear_output_loss: 1.1065 - val_pose_output_loss: 1.1552 - val_emotion_output_loss: 1.3714 - val_gender_output_acc: 0.6774 - val_image_quality_output_acc: 0.5025 - val_age_output_acc: 0.3422 - val_weight_output_acc: 0.5902 - val_bag_output_acc: 0.5348 - val_footwear_output_acc: 0.6003 - val_pose_output_acc: 0.7082 - val_emotion_output_acc: 0.4738\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 8/10\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.0143 - gender_output_loss: 0.0804 - image_quality_output_loss: 0.2853 - age_output_loss: 0.5120 - weight_output_loss: 0.2836 - bag_output_loss: 0.2078 - footwear_output_loss: 0.2392 - pose_output_loss: 0.0921 - emotion_output_loss: 0.3139 - gender_output_acc: 0.9688 - image_quality_output_acc: 0.8827 - age_output_acc: 0.7984 - weight_output_acc: 0.8901 - bag_output_acc: 0.9211 - footwear_output_acc: 0.9062 - pose_output_acc: 0.9660 - emotion_output_acc: 0.8857\n",
            "360/360 [==============================] - 45s 126ms/step - loss: 2.0165 - gender_output_loss: 0.0806 - image_quality_output_loss: 0.2851 - age_output_loss: 0.5124 - weight_output_loss: 0.2839 - bag_output_loss: 0.2078 - footwear_output_loss: 0.2396 - pose_output_loss: 0.0923 - emotion_output_loss: 0.3149 - gender_output_acc: 0.9687 - image_quality_output_acc: 0.8827 - age_output_acc: 0.7981 - weight_output_acc: 0.8898 - bag_output_acc: 0.9211 - footwear_output_acc: 0.9060 - pose_output_acc: 0.9660 - emotion_output_acc: 0.8853 - val_loss: 11.4957 - val_gender_output_loss: 0.9668 - val_image_quality_output_loss: 1.3833 - val_age_output_loss: 2.0873 - val_weight_output_loss: 1.9398 - val_bag_output_loss: 1.4048 - val_footwear_output_loss: 1.1925 - val_pose_output_loss: 1.0572 - val_emotion_output_loss: 1.4640 - val_gender_output_acc: 0.7641 - val_image_quality_output_acc: 0.4773 - val_age_output_acc: 0.2979 - val_weight_output_acc: 0.5862 - val_bag_output_acc: 0.5464 - val_footwear_output_acc: 0.6326 - val_pose_output_acc: 0.7213 - val_emotion_output_acc: 0.4879\n",
            "Epoch 9/10\n",
            "360/360 [==============================] - 45s 125ms/step - loss: 1.6040 - gender_output_loss: 0.0629 - image_quality_output_loss: 0.2380 - age_output_loss: 0.3971 - weight_output_loss: 0.2259 - bag_output_loss: 0.1666 - footwear_output_loss: 0.1936 - pose_output_loss: 0.0801 - emotion_output_loss: 0.2400 - gender_output_acc: 0.9756 - image_quality_output_acc: 0.9080 - age_output_acc: 0.8519 - weight_output_acc: 0.9141 - bag_output_acc: 0.9367 - footwear_output_acc: 0.9277 - pose_output_acc: 0.9718 - emotion_output_acc: 0.9131 - val_loss: 14.7502 - val_gender_output_loss: 1.3848 - val_image_quality_output_loss: 1.9239 - val_age_output_loss: 2.2733 - val_weight_output_loss: 2.5946 - val_bag_output_loss: 1.6010 - val_footwear_output_loss: 1.4232 - val_pose_output_loss: 1.7471 - val_emotion_output_loss: 1.8024 - val_gender_output_acc: 0.7278 - val_image_quality_output_acc: 0.4309 - val_age_output_acc: 0.3458 - val_weight_output_acc: 0.5877 - val_bag_output_acc: 0.5136 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.7001 - val_emotion_output_acc: 0.4461\n",
            "Epoch 10/10\n",
            "359/360 [============================>.] - ETA: 0s - loss: 1.1686 - gender_output_loss: 0.0539 - image_quality_output_loss: 0.1817 - age_output_loss: 0.2698 - weight_output_loss: 0.1657 - bag_output_loss: 0.1260 - footwear_output_loss: 0.1527 - pose_output_loss: 0.0563 - emotion_output_loss: 0.1624 - gender_output_acc: 0.9810 - image_quality_output_acc: 0.9272 - age_output_acc: 0.9026 - weight_output_acc: 0.9384 - bag_output_acc: 0.9513 - footwear_output_acc: 0.9441 - pose_output_acc: 0.9792 - emotion_output_acc: 0.9408Epoch 10/10\n",
            "360/360 [==============================] - 45s 125ms/step - loss: 1.1686 - gender_output_loss: 0.0543 - image_quality_output_loss: 0.1820 - age_output_loss: 0.2694 - weight_output_loss: 0.1658 - bag_output_loss: 0.1259 - footwear_output_loss: 0.1526 - pose_output_loss: 0.0562 - emotion_output_loss: 0.1625 - gender_output_acc: 0.9810 - image_quality_output_acc: 0.9273 - age_output_acc: 0.9028 - weight_output_acc: 0.9382 - bag_output_acc: 0.9515 - footwear_output_acc: 0.9442 - pose_output_acc: 0.9793 - emotion_output_acc: 0.9408 - val_loss: 16.6728 - val_gender_output_loss: 1.8873 - val_image_quality_output_loss: 3.5591 - val_age_output_loss: 2.5830 - val_weight_output_loss: 2.3863 - val_bag_output_loss: 1.7256 - val_footwear_output_loss: 1.4170 - val_pose_output_loss: 1.3874 - val_emotion_output_loss: 1.7271 - val_gender_output_acc: 0.6890 - val_image_quality_output_acc: 0.3402 - val_age_output_acc: 0.3261 - val_weight_output_acc: 0.6023 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5907 - val_pose_output_acc: 0.7182 - val_emotion_output_acc: 0.5091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdb46247cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "outputId": "22899937-fb91-4b12-8fa5-7715e4a49147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2a (Conv2D)         (None, 56, 56, 64)   4160        max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 56, 56, 64)   0           bn2a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 56, 56, 64)   0           bn2a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch1 (Conv2D)          (None, 56, 56, 256)  16640       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch1 (BatchNormalizatio (None, 56, 56, 256)  1024        res2a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 56, 56, 256)  0           bn2a_branch2c[0][0]              \n",
            "                                                                 bn2a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 56, 56, 256)  0           add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 56, 56, 64)   0           bn2b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 56, 56, 64)   0           bn2b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_34 (Add)                    (None, 56, 56, 256)  0           bn2b_branch2c[0][0]              \n",
            "                                                                 activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 56, 56, 256)  0           add_34[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 56, 56, 64)   0           bn2c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 56, 56, 64)   0           bn2c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_35 (Add)                    (None, 56, 56, 256)  0           bn2c_branch2c[0][0]              \n",
            "                                                                 activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 56, 56, 256)  0           add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2a (Conv2D)         (None, 28, 28, 128)  32896       activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 28, 28, 128)  0           bn3a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 28, 28, 128)  0           bn3a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch1 (Conv2D)          (None, 28, 28, 512)  131584      activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch1 (BatchNormalizatio (None, 28, 28, 512)  2048        res3a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_36 (Add)                    (None, 28, 28, 512)  0           bn3a_branch2c[0][0]              \n",
            "                                                                 bn3a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 28, 28, 512)  0           add_36[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 28, 28, 128)  0           bn3b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 28, 28, 128)  0           bn3b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 28, 28, 512)  0           bn3b_branch2c[0][0]              \n",
            "                                                                 activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 28, 28, 512)  0           add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 28, 28, 128)  0           bn3c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 28, 28, 128)  0           bn3c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 28, 28, 512)  0           bn3c_branch2c[0][0]              \n",
            "                                                                 activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 28, 28, 512)  0           add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 28, 28, 128)  0           bn3d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 28, 28, 128)  0           bn3d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 28, 28, 512)  0           bn3d_branch2c[0][0]              \n",
            "                                                                 activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 28, 28, 512)  0           add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n",
            "                                                                 bn4a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 14, 14, 1024) 0           add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n",
            "                                                                 activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 14, 14, 1024) 0           add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n",
            "                                                                 activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 14, 14, 1024) 0           add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n",
            "                                                                 activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 14, 14, 1024) 0           add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n",
            "                                                                 activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 14, 14, 1024) 0           add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n",
            "                                                                 activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 14, 14, 1024) 0           add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
            "                                                                 bn5a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 7, 7, 2048)   0           add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
            "                                                                 activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 7, 7, 2048)   0           add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
            "                                                                 activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 7, 7, 2048)   0           add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 100352)       0           activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          51380736    flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 256)          131328      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 256)          131328      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          131328      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 256)          131328      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 256)          131328      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 256)          131328      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 256)          131328      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 256)          131328      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          32896       dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          32896       dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          32896       dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          32896       dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 128)          32896       dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 128)          32896       dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 128)          32896       dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 128)          32896       dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_15[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 76,285,723\n",
            "Trainable params: 52,698,011\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WD13pzLO2Or",
        "colab_type": "text"
      },
      "source": [
        "## My Models\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV8KivfHOk-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OipiWaZZ_WIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_tower(in_layer):\n",
        "    #neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(256, activation=\"relu\")(in_layer)\n",
        "    #neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HgH8JShAFaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input image dimensions.\n",
        "input_shape = (224, 224, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ1KCaoM9jlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 16  # orig paper trained all networks with batch_size=128\n",
        "epochs = 200\n",
        "data_augmentation = False\n",
        "\n",
        "n = 9\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z9SN59z9qt_",
        "colab_type": "code",
        "outputId": "fcf2e5e2-9704-4228-a9e8-959bfddeb2ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    # outputs = Dense(num_classes,\n",
        "    #                 activation='softmax',\n",
        "    #                 kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # # Instantiate model.\n",
        "    # model = Model(inputs=inputs, outputs=outputs)\n",
        "    # return model\n",
        "\n",
        "    \n",
        "    neck = y\n",
        "    # heads\n",
        "    gender = build_head(\"gender\", build_tower(neck))\n",
        "    image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "    age = build_head(\"age\", build_tower(neck))\n",
        "    weight = build_head(\"weight\", build_tower(neck))\n",
        "    bag = build_head(\"bag\", build_tower(neck))\n",
        "    footwear = build_head(\"footwear\", build_tower(neck))\n",
        "    emotion = build_head(\"emotion\", build_tower(neck))\n",
        "    pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "    model = Model(\n",
        "        inputs=inputs,#backbone.input, \n",
        "        outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wdL7kwh-P7B",
        "colab_type": "code",
        "outputId": "4f4851a2-3235-4f3f-b94d-b453827ec561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = 'gdrive/My Drive/Assignment5/' + 'saved_models'\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 224, 224, 16) 448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 224, 224, 16) 64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 224, 224, 16) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 224, 224, 16) 2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 224, 224, 16) 64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 224, 224, 16) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 224, 224, 16) 2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 224, 224, 16) 64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 224, 224, 16) 0           activation_1[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 224, 224, 16) 0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 224, 224, 16) 2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 224, 224, 16) 64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 224, 224, 16) 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 224, 224, 16) 2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 224, 224, 16) 64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 224, 224, 16) 0           activation_3[0][0]               \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 224, 224, 16) 0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 224, 224, 16) 2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 224, 224, 16) 64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 224, 224, 16) 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 224, 224, 16) 2320        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 224, 224, 16) 64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 224, 224, 16) 0           activation_5[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 224, 224, 16) 0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 224, 224, 16) 2320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 224, 224, 16) 64          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 224, 224, 16) 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 224, 224, 16) 2320        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 224, 224, 16) 64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 224, 224, 16) 0           activation_7[0][0]               \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 224, 224, 16) 0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 224, 224, 16) 2320        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 224, 224, 16) 64          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 224, 224, 16) 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 224, 224, 16) 2320        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 224, 224, 16) 64          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 224, 224, 16) 0           activation_9[0][0]               \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 224, 224, 16) 0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 224, 224, 16) 2320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 224, 224, 16) 64          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 224, 224, 16) 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 224, 224, 16) 2320        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 224, 224, 16) 64          conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 224, 224, 16) 0           activation_11[0][0]              \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 224, 224, 16) 0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 224, 224, 16) 2320        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 224, 224, 16) 64          conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 224, 224, 16) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 224, 224, 16) 2320        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 224, 224, 16) 64          conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 224, 224, 16) 0           activation_13[0][0]              \n",
            "                                                                 batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 224, 224, 16) 0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 224, 224, 16) 2320        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 224, 224, 16) 64          conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 224, 224, 16) 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 224, 224, 16) 2320        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 224, 224, 16) 64          conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 224, 224, 16) 0           activation_15[0][0]              \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 224, 224, 16) 0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 224, 224, 16) 2320        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 224, 224, 16) 64          conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 224, 224, 16) 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 224, 224, 16) 2320        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 224, 224, 16) 64          conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 224, 224, 16) 0           activation_17[0][0]              \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 224, 224, 16) 0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 112, 112, 32) 4640        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 112, 112, 32) 128         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 112, 112, 32) 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 112, 112, 32) 9248        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 112, 112, 32) 544         activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 112, 112, 32) 128         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 112, 112, 32) 0           conv2d_22[0][0]                  \n",
            "                                                                 batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 112, 112, 32) 0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 112, 112, 32) 9248        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 112, 112, 32) 128         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 112, 112, 32) 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 112, 112, 32) 9248        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 112, 112, 32) 128         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 112, 112, 32) 0           activation_21[0][0]              \n",
            "                                                                 batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 112, 112, 32) 0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 112, 112, 32) 9248        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 112, 112, 32) 128         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 112, 112, 32) 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 112, 112, 32) 9248        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 112, 112, 32) 128         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 112, 112, 32) 0           activation_23[0][0]              \n",
            "                                                                 batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 112, 112, 32) 0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 112, 112, 32) 9248        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 112, 112, 32) 128         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 112, 112, 32) 0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 112, 112, 32) 9248        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 112, 112, 32) 128         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 112, 112, 32) 0           activation_25[0][0]              \n",
            "                                                                 batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 112, 112, 32) 0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 112, 112, 32) 9248        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 112, 112, 32) 128         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 112, 112, 32) 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 112, 112, 32) 9248        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 112, 112, 32) 128         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 112, 112, 32) 0           activation_27[0][0]              \n",
            "                                                                 batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 112, 112, 32) 0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 112, 112, 32) 9248        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 112, 112, 32) 128         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 112, 112, 32) 0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 112, 112, 32) 9248        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 112, 112, 32) 128         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 112, 112, 32) 0           activation_29[0][0]              \n",
            "                                                                 batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 112, 112, 32) 0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 112, 112, 32) 9248        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 112, 112, 32) 128         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 112, 112, 32) 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 112, 112, 32) 9248        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 112, 112, 32) 128         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 112, 112, 32) 0           activation_31[0][0]              \n",
            "                                                                 batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 112, 112, 32) 0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 112, 112, 32) 9248        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 112, 112, 32) 128         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 112, 112, 32) 0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 112, 112, 32) 9248        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 112, 112, 32) 128         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 112, 112, 32) 0           activation_33[0][0]              \n",
            "                                                                 batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 112, 112, 32) 0           add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 112, 112, 32) 9248        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 112, 112, 32) 128         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 112, 112, 32) 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 112, 112, 32) 9248        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 112, 112, 32) 128         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 112, 112, 32) 0           activation_35[0][0]              \n",
            "                                                                 batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 112, 112, 32) 0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 56, 56, 64)   18496       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 56, 56, 64)   256         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 56, 56, 64)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 56, 56, 64)   36928       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 56, 56, 64)   2112        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 56, 56, 64)   256         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 56, 56, 64)   0           conv2d_41[0][0]                  \n",
            "                                                                 batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 56, 56, 64)   0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 56, 56, 64)   36928       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 56, 56, 64)   256         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 56, 56, 64)   0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 56, 56, 64)   36928       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 56, 56, 64)   256         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 56, 56, 64)   0           activation_39[0][0]              \n",
            "                                                                 batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 56, 56, 64)   0           add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 56, 56, 64)   36928       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 56, 56, 64)   256         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 56, 56, 64)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 56, 56, 64)   36928       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 56, 56, 64)   256         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 56, 56, 64)   0           activation_41[0][0]              \n",
            "                                                                 batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 56, 56, 64)   0           add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 56, 56, 64)   36928       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 56, 56, 64)   256         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 56, 56, 64)   0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 56, 56, 64)   36928       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 56, 56, 64)   256         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 56, 56, 64)   0           activation_43[0][0]              \n",
            "                                                                 batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 56, 56, 64)   0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 56, 56, 64)   36928       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 56, 56, 64)   256         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 56, 56, 64)   0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 56, 56, 64)   36928       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 56, 56, 64)   256         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 56, 56, 64)   0           activation_45[0][0]              \n",
            "                                                                 batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 56, 56, 64)   0           add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 56, 56, 64)   36928       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 56, 56, 64)   256         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 56, 56, 64)   0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 56, 56, 64)   36928       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 56, 56, 64)   256         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 56, 56, 64)   0           activation_47[0][0]              \n",
            "                                                                 batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 56, 56, 64)   0           add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 56, 56, 64)   36928       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 56, 56, 64)   256         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 56, 56, 64)   0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 56, 56, 64)   36928       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 56, 56, 64)   256         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 56, 56, 64)   0           activation_49[0][0]              \n",
            "                                                                 batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 56, 56, 64)   0           add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 56, 56, 64)   36928       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 56, 56, 64)   256         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 56, 56, 64)   0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 56, 56, 64)   36928       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 56, 56, 64)   256         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 56, 56, 64)   0           activation_51[0][0]              \n",
            "                                                                 batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 56, 56, 64)   0           add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 56, 56, 64)   36928       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 56, 56, 64)   256         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 56, 56, 64)   0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 56, 56, 64)   36928       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 56, 56, 64)   256         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 56, 56, 64)   0           activation_53[0][0]              \n",
            "                                                                 batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 56, 56, 64)   0           add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 7, 7, 64)     0           activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 3136)         0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          803072      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          803072      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 256)          803072      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 256)          803072      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 256)          803072      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 256)          803072      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 256)          803072      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 256)          803072      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          32896       dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          32896       dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          32896       dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 128)          32896       dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          32896       dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 128)          32896       dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 128)          32896       dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 7,552,347\n",
            "Trainable params: 7,548,283\n",
            "Non-trainable params: 4,064\n",
            "__________________________________________________________________________________________________\n",
            "ResNet56v1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg6c9ADqA1DI",
        "colab_type": "code",
        "outputId": "9af99ae5-fdc8-4eaa-ca5e-d6449f41d8f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 242s 672ms/step - loss: 8.6382 - gender_output_loss: 0.6965 - image_quality_output_loss: 1.0615 - age_output_loss: 1.4833 - weight_output_loss: 1.0953 - bag_output_loss: 0.9626 - footwear_output_loss: 0.9881 - pose_output_loss: 0.9587 - emotion_output_loss: 0.9676 - gender_output_acc: 0.5769 - image_quality_output_acc: 0.5379 - age_output_acc: 0.3826 - weight_output_acc: 0.6172 - bag_output_acc: 0.5485 - footwear_output_acc: 0.5411 - pose_output_acc: 0.6082 - emotion_output_acc: 0.7071 - val_loss: 8.0532 - val_gender_output_loss: 0.6349 - val_image_quality_output_loss: 0.9756 - val_age_output_loss: 1.4104 - val_weight_output_loss: 0.9496 - val_bag_output_loss: 0.9131 - val_footwear_output_loss: 0.9131 - val_pose_output_loss: 0.9167 - val_emotion_output_loss: 0.9154 - val_gender_output_acc: 0.6285 - val_image_quality_output_acc: 0.5524 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6583 - val_bag_output_acc: 0.5494 - val_footwear_output_acc: 0.5756 - val_pose_output_acc: 0.6134 - val_emotion_output_acc: 0.7067\n",
            "Epoch 2/200\n",
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "360/360 [==============================] - 222s 617ms/step - loss: 7.9986 - gender_output_loss: 0.6343 - image_quality_output_loss: 0.9340 - age_output_loss: 1.4105 - weight_output_loss: 0.9902 - bag_output_loss: 0.9077 - footwear_output_loss: 0.9065 - pose_output_loss: 0.8849 - emotion_output_loss: 0.9062 - gender_output_acc: 0.6226 - image_quality_output_acc: 0.5520 - age_output_acc: 0.3927 - weight_output_acc: 0.6305 - bag_output_acc: 0.5645 - footwear_output_acc: 0.5870 - pose_output_acc: 0.6234 - emotion_output_acc: 0.7124 - val_loss: 7.9729 - val_gender_output_loss: 0.6219 - val_image_quality_output_loss: 0.9357 - val_age_output_loss: 1.4151 - val_weight_output_loss: 0.9422 - val_bag_output_loss: 0.9003 - val_footwear_output_loss: 0.8956 - val_pose_output_loss: 0.9198 - val_emotion_output_loss: 0.9185 - val_gender_output_acc: 0.6431 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.3911 - val_weight_output_acc: 0.6583 - val_bag_output_acc: 0.5630 - val_footwear_output_acc: 0.5832 - val_pose_output_acc: 0.5948 - val_emotion_output_acc: 0.7061\n",
            "Epoch 3/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 7.8517 - gender_output_loss: 0.6090 - image_quality_output_loss: 0.9191 - age_output_loss: 1.4040 - weight_output_loss: 0.9804 - bag_output_loss: 0.8959 - footwear_output_loss: 0.8776 - pose_output_loss: 0.8436 - emotion_output_loss: 0.8984 - gender_output_acc: 0.6564 - image_quality_output_acc: 0.5534 - age_output_acc: 0.3961 - weight_output_acc: 0.6318 - bag_output_acc: 0.5677 - footwear_output_acc: 0.6001 - pose_output_acc: 0.6396 - emotion_output_acc: 0.7126 - val_loss: 7.8631 - val_gender_output_loss: 0.5996 - val_image_quality_output_loss: 0.9300 - val_age_output_loss: 1.4105 - val_weight_output_loss: 0.9738 - val_bag_output_loss: 0.8863 - val_footwear_output_loss: 0.8796 - val_pose_output_loss: 0.8535 - val_emotion_output_loss: 0.9065 - val_gender_output_acc: 0.6648 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.4012 - val_weight_output_acc: 0.6613 - val_bag_output_acc: 0.5736 - val_footwear_output_acc: 0.5907 - val_pose_output_acc: 0.6497 - val_emotion_output_acc: 0.7056\n",
            "Epoch 4/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 7.7452 - gender_output_loss: 0.5928 - image_quality_output_loss: 0.9060 - age_output_loss: 1.3988 - weight_output_loss: 0.9765 - bag_output_loss: 0.8868 - footwear_output_loss: 0.8570 - pose_output_loss: 0.8095 - emotion_output_loss: 0.8948 - gender_output_acc: 0.6745 - image_quality_output_acc: 0.5604 - age_output_acc: 0.4003 - weight_output_acc: 0.6319 - bag_output_acc: 0.5813 - footwear_output_acc: 0.6088 - pose_output_acc: 0.6522 - emotion_output_acc: 0.7122 - val_loss: 7.8295 - val_gender_output_loss: 0.5972 - val_image_quality_output_loss: 0.9120 - val_age_output_loss: 1.3953 - val_weight_output_loss: 0.9423 - val_bag_output_loss: 0.8994 - val_footwear_output_loss: 0.9101 - val_pose_output_loss: 0.8310 - val_emotion_output_loss: 0.9195 - val_gender_output_acc: 0.6724 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6598 - val_bag_output_acc: 0.5645 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.6517 - val_emotion_output_acc: 0.7061\n",
            "Epoch 5/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 7.6378 - gender_output_loss: 0.5708 - image_quality_output_loss: 0.9034 - age_output_loss: 1.3900 - weight_output_loss: 0.9729 - bag_output_loss: 0.8773 - footwear_output_loss: 0.8343 - pose_output_loss: 0.7769 - emotion_output_loss: 0.8896 - gender_output_acc: 0.6960 - image_quality_output_acc: 0.5594 - age_output_acc: 0.4016 - weight_output_acc: 0.6324 - bag_output_acc: 0.5843 - footwear_output_acc: 0.6257 - pose_output_acc: 0.6714 - emotion_output_acc: 0.7124 - val_loss: 7.7402 - val_gender_output_loss: 0.5968 - val_image_quality_output_loss: 0.9140 - val_age_output_loss: 1.4074 - val_weight_output_loss: 0.9538 - val_bag_output_loss: 0.8888 - val_footwear_output_loss: 0.8523 - val_pose_output_loss: 0.7985 - val_emotion_output_loss: 0.9064 - val_gender_output_acc: 0.6855 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3921 - val_weight_output_acc: 0.6578 - val_bag_output_acc: 0.5680 - val_footwear_output_acc: 0.6169 - val_pose_output_acc: 0.6568 - val_emotion_output_acc: 0.7061\n",
            "Epoch 6/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 7.5399 - gender_output_loss: 0.5622 - image_quality_output_loss: 0.8920 - age_output_loss: 1.3844 - weight_output_loss: 0.9668 - bag_output_loss: 0.8701 - footwear_output_loss: 0.8187 - pose_output_loss: 0.7387 - emotion_output_loss: 0.8849 - gender_output_acc: 0.7024 - image_quality_output_acc: 0.5672 - age_output_acc: 0.4010 - weight_output_acc: 0.6327 - bag_output_acc: 0.5927 - footwear_output_acc: 0.6303 - pose_output_acc: 0.6895 - emotion_output_acc: 0.7128 - val_loss: 7.7125 - val_gender_output_loss: 0.5943 - val_image_quality_output_loss: 0.9181 - val_age_output_loss: 1.4066 - val_weight_output_loss: 0.9323 - val_bag_output_loss: 0.8864 - val_footwear_output_loss: 0.8600 - val_pose_output_loss: 0.7698 - val_emotion_output_loss: 0.9233 - val_gender_output_acc: 0.6981 - val_image_quality_output_acc: 0.5504 - val_age_output_acc: 0.3906 - val_weight_output_acc: 0.6598 - val_bag_output_acc: 0.5932 - val_footwear_output_acc: 0.6174 - val_pose_output_acc: 0.6784 - val_emotion_output_acc: 0.7016\n",
            "Epoch 7/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 7.4225 - gender_output_loss: 0.5452 - image_quality_output_loss: 0.8862 - age_output_loss: 1.3754 - weight_output_loss: 0.9591 - bag_output_loss: 0.8591 - footwear_output_loss: 0.8000 - pose_output_loss: 0.6990 - emotion_output_loss: 0.8770 - gender_output_acc: 0.7201 - image_quality_output_acc: 0.5723 - age_output_acc: 0.4060 - weight_output_acc: 0.6345 - bag_output_acc: 0.5970 - footwear_output_acc: 0.6386 - pose_output_acc: 0.7034 - emotion_output_acc: 0.7133 - val_loss: 7.6534 - val_gender_output_loss: 0.5892 - val_image_quality_output_loss: 0.9333 - val_age_output_loss: 1.4009 - val_weight_output_loss: 0.9318 - val_bag_output_loss: 0.8927 - val_footwear_output_loss: 0.8461 - val_pose_output_loss: 0.7187 - val_emotion_output_loss: 0.9195 - val_gender_output_acc: 0.6961 - val_image_quality_output_acc: 0.5469 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6603 - val_bag_output_acc: 0.5721 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.6981 - val_emotion_output_acc: 0.7041\n",
            "Epoch 8/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 7.3040 - gender_output_loss: 0.5189 - image_quality_output_loss: 0.8841 - age_output_loss: 1.3665 - weight_output_loss: 0.9541 - bag_output_loss: 0.8543 - footwear_output_loss: 0.7882 - pose_output_loss: 0.6486 - emotion_output_loss: 0.8684 - gender_output_acc: 0.7367 - image_quality_output_acc: 0.5697 - age_output_acc: 0.4096 - weight_output_acc: 0.6310 - bag_output_acc: 0.6034 - footwear_output_acc: 0.6468 - pose_output_acc: 0.7306 - emotion_output_acc: 0.7134 - val_loss: 7.5566 - val_gender_output_loss: 0.5361 - val_image_quality_output_loss: 0.9633 - val_age_output_loss: 1.3961 - val_weight_output_loss: 0.9284 - val_bag_output_loss: 0.8813 - val_footwear_output_loss: 0.8403 - val_pose_output_loss: 0.6827 - val_emotion_output_loss: 0.9077 - val_gender_output_acc: 0.7374 - val_image_quality_output_acc: 0.5358 - val_age_output_acc: 0.3987 - val_weight_output_acc: 0.6573 - val_bag_output_acc: 0.5796 - val_footwear_output_acc: 0.6421 - val_pose_output_acc: 0.7253 - val_emotion_output_acc: 0.7011\n",
            "Epoch 9/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 7.2087 - gender_output_loss: 0.5090 - image_quality_output_loss: 0.8777 - age_output_loss: 1.3568 - weight_output_loss: 0.9444 - bag_output_loss: 0.8419 - footwear_output_loss: 0.7733 - pose_output_loss: 0.6221 - emotion_output_loss: 0.8630 - gender_output_acc: 0.7465 - image_quality_output_acc: 0.5818 - age_output_acc: 0.4091 - weight_output_acc: 0.6345 - bag_output_acc: 0.6102 - footwear_output_acc: 0.6556 - pose_output_acc: 0.7391 - emotion_output_acc: 0.7135 - val_loss: 7.5438 - val_gender_output_loss: 0.5332 - val_image_quality_output_loss: 0.9637 - val_age_output_loss: 1.3926 - val_weight_output_loss: 0.9280 - val_bag_output_loss: 0.8802 - val_footwear_output_loss: 0.8382 - val_pose_output_loss: 0.6790 - val_emotion_output_loss: 0.9087 - val_gender_output_acc: 0.7298 - val_image_quality_output_acc: 0.5373 - val_age_output_acc: 0.3821 - val_weight_output_acc: 0.6578 - val_bag_output_acc: 0.5817 - val_footwear_output_acc: 0.6331 - val_pose_output_acc: 0.7147 - val_emotion_output_acc: 0.7056\n",
            "Epoch 10/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 7.0937 - gender_output_loss: 0.4854 - image_quality_output_loss: 0.8698 - age_output_loss: 1.3483 - weight_output_loss: 0.9384 - bag_output_loss: 0.8290 - footwear_output_loss: 0.7616 - pose_output_loss: 0.5877 - emotion_output_loss: 0.8535 - gender_output_acc: 0.7602 - image_quality_output_acc: 0.5889 - age_output_acc: 0.4169 - weight_output_acc: 0.6364 - bag_output_acc: 0.6186 - footwear_output_acc: 0.6614 - pose_output_acc: 0.7556 - emotion_output_acc: 0.7135 - val_loss: 7.6657 - val_gender_output_loss: 0.5783 - val_image_quality_output_loss: 0.9405 - val_age_output_loss: 1.3935 - val_weight_output_loss: 0.9377 - val_bag_output_loss: 0.8952 - val_footwear_output_loss: 0.8714 - val_pose_output_loss: 0.7030 - val_emotion_output_loss: 0.9265 - val_gender_output_acc: 0.6986 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.3851 - val_weight_output_acc: 0.6532 - val_bag_output_acc: 0.5691 - val_footwear_output_acc: 0.6159 - val_pose_output_acc: 0.7051 - val_emotion_output_acc: 0.7006\n",
            "Epoch 11/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 225s 624ms/step - loss: 6.9784 - gender_output_loss: 0.4631 - image_quality_output_loss: 0.8594 - age_output_loss: 1.3386 - weight_output_loss: 0.9291 - bag_output_loss: 0.8194 - footwear_output_loss: 0.7490 - pose_output_loss: 0.5560 - emotion_output_loss: 0.8443 - gender_output_acc: 0.7773 - image_quality_output_acc: 0.5910 - age_output_acc: 0.4178 - weight_output_acc: 0.6404 - bag_output_acc: 0.6223 - footwear_output_acc: 0.6640 - pose_output_acc: 0.7714 - emotion_output_acc: 0.7151 - val_loss: 7.6630 - val_gender_output_loss: 0.5600 - val_image_quality_output_loss: 0.9262 - val_age_output_loss: 1.4210 - val_weight_output_loss: 0.9661 - val_bag_output_loss: 0.8867 - val_footwear_output_loss: 0.8558 - val_pose_output_loss: 0.6939 - val_emotion_output_loss: 0.9340 - val_gender_output_acc: 0.7344 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.6154 - val_pose_output_acc: 0.7258 - val_emotion_output_acc: 0.6976\n",
            "Epoch 12/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 6.8787 - gender_output_loss: 0.4441 - image_quality_output_loss: 0.8518 - age_output_loss: 1.3304 - weight_output_loss: 0.9200 - bag_output_loss: 0.8058 - footwear_output_loss: 0.7408 - pose_output_loss: 0.5340 - emotion_output_loss: 0.8329 - gender_output_acc: 0.7918 - image_quality_output_acc: 0.5961 - age_output_acc: 0.4252 - weight_output_acc: 0.6397 - bag_output_acc: 0.6335 - footwear_output_acc: 0.6698 - pose_output_acc: 0.7793 - emotion_output_acc: 0.7168 - val_loss: 7.6828 - val_gender_output_loss: 0.5321 - val_image_quality_output_loss: 0.9464 - val_age_output_loss: 1.4194 - val_weight_output_loss: 0.9475 - val_bag_output_loss: 0.9162 - val_footwear_output_loss: 0.8593 - val_pose_output_loss: 0.7051 - val_emotion_output_loss: 0.9379 - val_gender_output_acc: 0.7475 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3846 - val_weight_output_acc: 0.6578 - val_bag_output_acc: 0.5852 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.7268 - val_emotion_output_acc: 0.7051\n",
            "Epoch 13/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 623ms/step - loss: 6.7505 - gender_output_loss: 0.4219 - image_quality_output_loss: 0.8444 - age_output_loss: 1.3133 - weight_output_loss: 0.9104 - bag_output_loss: 0.7896 - footwear_output_loss: 0.7272 - pose_output_loss: 0.5045 - emotion_output_loss: 0.8208 - gender_output_acc: 0.8021 - image_quality_output_acc: 0.6026 - age_output_acc: 0.4264 - weight_output_acc: 0.6407 - bag_output_acc: 0.6445 - footwear_output_acc: 0.6787 - pose_output_acc: 0.7931 - emotion_output_acc: 0.7188 - val_loss: 8.0245 - val_gender_output_loss: 0.5899 - val_image_quality_output_loss: 0.9470 - val_age_output_loss: 1.4631 - val_weight_output_loss: 0.9584 - val_bag_output_loss: 0.9692 - val_footwear_output_loss: 0.8912 - val_pose_output_loss: 0.7717 - val_emotion_output_loss: 1.0156 - val_gender_output_acc: 0.7167 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.3871 - val_weight_output_acc: 0.6527 - val_bag_output_acc: 0.5963 - val_footwear_output_acc: 0.6205 - val_pose_output_acc: 0.6920 - val_emotion_output_acc: 0.7041\n",
            "Epoch 14/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 623ms/step - loss: 6.6083 - gender_output_loss: 0.4007 - image_quality_output_loss: 0.8321 - age_output_loss: 1.3004 - weight_output_loss: 0.8932 - bag_output_loss: 0.7702 - footwear_output_loss: 0.7075 - pose_output_loss: 0.4803 - emotion_output_loss: 0.8058 - gender_output_acc: 0.8120 - image_quality_output_acc: 0.6095 - age_output_acc: 0.4365 - weight_output_acc: 0.6471 - bag_output_acc: 0.6546 - footwear_output_acc: 0.6855 - pose_output_acc: 0.8069 - emotion_output_acc: 0.7202 - val_loss: 8.0422 - val_gender_output_loss: 0.6150 - val_image_quality_output_loss: 1.0767 - val_age_output_loss: 1.4262 - val_weight_output_loss: 0.9756 - val_bag_output_loss: 0.9186 - val_footwear_output_loss: 0.8946 - val_pose_output_loss: 0.7419 - val_emotion_output_loss: 0.9756 - val_gender_output_acc: 0.7046 - val_image_quality_output_acc: 0.4733 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6114 - val_pose_output_acc: 0.7072 - val_emotion_output_acc: 0.6744\n",
            "Epoch 15/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 623ms/step - loss: 6.4717 - gender_output_loss: 0.3825 - image_quality_output_loss: 0.8185 - age_output_loss: 1.2815 - weight_output_loss: 0.8804 - bag_output_loss: 0.7549 - footwear_output_loss: 0.6957 - pose_output_loss: 0.4553 - emotion_output_loss: 0.7852 - gender_output_acc: 0.8267 - image_quality_output_acc: 0.6145 - age_output_acc: 0.4482 - weight_output_acc: 0.6513 - bag_output_acc: 0.6661 - footwear_output_acc: 0.6944 - pose_output_acc: 0.8199 - emotion_output_acc: 0.7233 - val_loss: 7.6798 - val_gender_output_loss: 0.5210 - val_image_quality_output_loss: 0.9850 - val_age_output_loss: 1.3998 - val_weight_output_loss: 0.9582 - val_bag_output_loss: 0.9061 - val_footwear_output_loss: 0.8380 - val_pose_output_loss: 0.7006 - val_emotion_output_loss: 0.9535 - val_gender_output_acc: 0.7545 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.3926 - val_weight_output_acc: 0.6487 - val_bag_output_acc: 0.5801 - val_footwear_output_acc: 0.6321 - val_pose_output_acc: 0.7248 - val_emotion_output_acc: 0.6870\n",
            "Epoch 16/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 622ms/step - loss: 6.3215 - gender_output_loss: 0.3680 - image_quality_output_loss: 0.8043 - age_output_loss: 1.2529 - weight_output_loss: 0.8646 - bag_output_loss: 0.7291 - footwear_output_loss: 0.6774 - pose_output_loss: 0.4328 - emotion_output_loss: 0.7750 - gender_output_acc: 0.8319 - image_quality_output_acc: 0.6275 - age_output_acc: 0.4624 - weight_output_acc: 0.6577 - bag_output_acc: 0.6799 - footwear_output_acc: 0.7055 - pose_output_acc: 0.8315 - emotion_output_acc: 0.7264 - val_loss: 8.6154 - val_gender_output_loss: 0.7036 - val_image_quality_output_loss: 1.0751 - val_age_output_loss: 1.5716 - val_weight_output_loss: 1.0295 - val_bag_output_loss: 1.1672 - val_footwear_output_loss: 0.9249 - val_pose_output_loss: 0.7464 - val_emotion_output_loss: 0.9799 - val_gender_output_acc: 0.6653 - val_image_quality_output_acc: 0.4874 - val_age_output_acc: 0.3649 - val_weight_output_acc: 0.6507 - val_bag_output_acc: 0.4889 - val_footwear_output_acc: 0.5781 - val_pose_output_acc: 0.7016 - val_emotion_output_acc: 0.6930\n",
            "Epoch 17/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 623ms/step - loss: 6.1337 - gender_output_loss: 0.3336 - image_quality_output_loss: 0.7861 - age_output_loss: 1.2388 - weight_output_loss: 0.8464 - bag_output_loss: 0.6980 - footwear_output_loss: 0.6573 - pose_output_loss: 0.4078 - emotion_output_loss: 0.7487 - gender_output_acc: 0.8542 - image_quality_output_acc: 0.6363 - age_output_acc: 0.4704 - weight_output_acc: 0.6648 - bag_output_acc: 0.7008 - footwear_output_acc: 0.7147 - pose_output_acc: 0.8372 - emotion_output_acc: 0.7349 - val_loss: 8.3087 - val_gender_output_loss: 0.5869 - val_image_quality_output_loss: 1.0187 - val_age_output_loss: 1.5030 - val_weight_output_loss: 1.0611 - val_bag_output_loss: 0.9866 - val_footwear_output_loss: 0.9068 - val_pose_output_loss: 0.7774 - val_emotion_output_loss: 1.0514 - val_gender_output_acc: 0.7384 - val_image_quality_output_acc: 0.5524 - val_age_output_acc: 0.3816 - val_weight_output_acc: 0.5867 - val_bag_output_acc: 0.5786 - val_footwear_output_acc: 0.6260 - val_pose_output_acc: 0.7102 - val_emotion_output_acc: 0.6562\n",
            "Epoch 18/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 5.9534 - gender_output_loss: 0.3077 - image_quality_output_loss: 0.7705 - age_output_loss: 1.2137 - weight_output_loss: 0.8237 - bag_output_loss: 0.6759 - footwear_output_loss: 0.6341 - pose_output_loss: 0.3779 - emotion_output_loss: 0.7332 - gender_output_acc: 0.8658 - image_quality_output_acc: 0.6467 - age_output_acc: 0.4870 - weight_output_acc: 0.6714 - bag_output_acc: 0.7063 - footwear_output_acc: 0.7242 - pose_output_acc: 0.8510 - emotion_output_acc: 0.7394 - val_loss: 8.4380 - val_gender_output_loss: 0.6252 - val_image_quality_output_loss: 1.0267 - val_age_output_loss: 1.5278 - val_weight_output_loss: 1.0366 - val_bag_output_loss: 1.0358 - val_footwear_output_loss: 0.9292 - val_pose_output_loss: 0.8219 - val_emotion_output_loss: 1.0182 - val_gender_output_acc: 0.7208 - val_image_quality_output_acc: 0.5363 - val_age_output_acc: 0.3372 - val_weight_output_acc: 0.6542 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.6174 - val_pose_output_acc: 0.7051 - val_emotion_output_acc: 0.6840\n",
            "Epoch 19/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 5.7324 - gender_output_loss: 0.2818 - image_quality_output_loss: 0.7482 - age_output_loss: 1.1829 - weight_output_loss: 0.7961 - bag_output_loss: 0.6453 - footwear_output_loss: 0.6128 - pose_output_loss: 0.3500 - emotion_output_loss: 0.6989 - gender_output_acc: 0.8773 - image_quality_output_acc: 0.6596 - age_output_acc: 0.4964 - weight_output_acc: 0.6819 - bag_output_acc: 0.7225 - footwear_output_acc: 0.7352 - pose_output_acc: 0.8654 - emotion_output_acc: 0.7497 - val_loss: 8.3409 - val_gender_output_loss: 0.7345 - val_image_quality_output_loss: 1.0285 - val_age_output_loss: 1.4635 - val_weight_output_loss: 0.9944 - val_bag_output_loss: 1.0023 - val_footwear_output_loss: 0.8902 - val_pose_output_loss: 0.8058 - val_emotion_output_loss: 1.0052 - val_gender_output_acc: 0.6855 - val_image_quality_output_acc: 0.5227 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6492 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.6290 - val_pose_output_acc: 0.7228 - val_emotion_output_acc: 0.6769\n",
            "Epoch 20/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 5.5085 - gender_output_loss: 0.2550 - image_quality_output_loss: 0.7251 - age_output_loss: 1.1494 - weight_output_loss: 0.7683 - bag_output_loss: 0.6096 - footwear_output_loss: 0.5873 - pose_output_loss: 0.3246 - emotion_output_loss: 0.6730 - gender_output_acc: 0.8938 - image_quality_output_acc: 0.6737 - age_output_acc: 0.5163 - weight_output_acc: 0.6960 - bag_output_acc: 0.7405 - footwear_output_acc: 0.7496 - pose_output_acc: 0.8727 - emotion_output_acc: 0.7587 - val_loss: 8.7863 - val_gender_output_loss: 0.6497 - val_image_quality_output_loss: 1.0505 - val_age_output_loss: 1.5465 - val_weight_output_loss: 1.0665 - val_bag_output_loss: 1.0706 - val_footwear_output_loss: 0.9822 - val_pose_output_loss: 0.9215 - val_emotion_output_loss: 1.0828 - val_gender_output_acc: 0.7450 - val_image_quality_output_acc: 0.5050 - val_age_output_acc: 0.3322 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5484 - val_footwear_output_acc: 0.6144 - val_pose_output_acc: 0.7147 - val_emotion_output_acc: 0.6547\n",
            "Epoch 21/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 5.2783 - gender_output_loss: 0.2391 - image_quality_output_loss: 0.6930 - age_output_loss: 1.1122 - weight_output_loss: 0.7383 - bag_output_loss: 0.5784 - footwear_output_loss: 0.5514 - pose_output_loss: 0.3005 - emotion_output_loss: 0.6495 - gender_output_acc: 0.8991 - image_quality_output_acc: 0.6877 - age_output_acc: 0.5316 - weight_output_acc: 0.7087 - bag_output_acc: 0.7559 - footwear_output_acc: 0.7646 - pose_output_acc: 0.8849 - emotion_output_acc: 0.7602 - val_loss: 8.6840 - val_gender_output_loss: 0.6538 - val_image_quality_output_loss: 1.0506 - val_age_output_loss: 1.5165 - val_weight_output_loss: 1.0670 - val_bag_output_loss: 1.0689 - val_footwear_output_loss: 0.9681 - val_pose_output_loss: 0.8673 - val_emotion_output_loss: 1.0759 - val_gender_output_acc: 0.7470 - val_image_quality_output_acc: 0.5328 - val_age_output_acc: 0.3422 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5857 - val_footwear_output_acc: 0.6139 - val_pose_output_acc: 0.6850 - val_emotion_output_acc: 0.6578\n",
            "Epoch 22/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 5.0334 - gender_output_loss: 0.2117 - image_quality_output_loss: 0.6752 - age_output_loss: 1.0718 - weight_output_loss: 0.7064 - bag_output_loss: 0.5420 - footwear_output_loss: 0.5304 - pose_output_loss: 0.2688 - emotion_output_loss: 0.6114 - gender_output_acc: 0.9145 - image_quality_output_acc: 0.6986 - age_output_acc: 0.5547 - weight_output_acc: 0.7201 - bag_output_acc: 0.7710 - footwear_output_acc: 0.7752 - pose_output_acc: 0.8959 - emotion_output_acc: 0.7713 - val_loss: 9.3174 - val_gender_output_loss: 0.7593 - val_image_quality_output_loss: 1.1305 - val_age_output_loss: 1.6210 - val_weight_output_loss: 1.1289 - val_bag_output_loss: 1.1495 - val_footwear_output_loss: 1.0189 - val_pose_output_loss: 0.9620 - val_emotion_output_loss: 1.1316 - val_gender_output_acc: 0.7193 - val_image_quality_output_acc: 0.4879 - val_age_output_acc: 0.3594 - val_weight_output_acc: 0.6154 - val_bag_output_acc: 0.5554 - val_footwear_output_acc: 0.6179 - val_pose_output_acc: 0.7067 - val_emotion_output_acc: 0.6714\n",
            "Epoch 23/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 4.7754 - gender_output_loss: 0.1933 - image_quality_output_loss: 0.6376 - age_output_loss: 1.0297 - weight_output_loss: 0.6694 - bag_output_loss: 0.5010 - footwear_output_loss: 0.5075 - pose_output_loss: 0.2452 - emotion_output_loss: 0.5761 - gender_output_acc: 0.9228 - image_quality_output_acc: 0.7174 - age_output_acc: 0.5764 - weight_output_acc: 0.7332 - bag_output_acc: 0.7935 - footwear_output_acc: 0.7836 - pose_output_acc: 0.9089 - emotion_output_acc: 0.7900 - val_loss: 9.5921 - val_gender_output_loss: 0.7651 - val_image_quality_output_loss: 1.1094 - val_age_output_loss: 1.6387 - val_weight_output_loss: 1.1978 - val_bag_output_loss: 1.2302 - val_footwear_output_loss: 1.0792 - val_pose_output_loss: 0.9865 - val_emotion_output_loss: 1.1695 - val_gender_output_acc: 0.7288 - val_image_quality_output_acc: 0.5363 - val_age_output_acc: 0.3493 - val_weight_output_acc: 0.5081 - val_bag_output_acc: 0.5504 - val_footwear_output_acc: 0.6064 - val_pose_output_acc: 0.7097 - val_emotion_output_acc: 0.6114\n",
            "Epoch 24/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 4.4979 - gender_output_loss: 0.1725 - image_quality_output_loss: 0.6077 - age_output_loss: 0.9781 - weight_output_loss: 0.6285 - bag_output_loss: 0.4693 - footwear_output_loss: 0.4670 - pose_output_loss: 0.2253 - emotion_output_loss: 0.5340 - gender_output_acc: 0.9319 - image_quality_output_acc: 0.7297 - age_output_acc: 0.5988 - weight_output_acc: 0.7501 - bag_output_acc: 0.8056 - footwear_output_acc: 0.8059 - pose_output_acc: 0.9180 - emotion_output_acc: 0.8030 - val_loss: 10.2019 - val_gender_output_loss: 0.7876 - val_image_quality_output_loss: 1.2069 - val_age_output_loss: 1.7912 - val_weight_output_loss: 1.2647 - val_bag_output_loss: 1.2911 - val_footwear_output_loss: 1.1167 - val_pose_output_loss: 1.0521 - val_emotion_output_loss: 1.2763 - val_gender_output_acc: 0.7450 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.3498 - val_weight_output_acc: 0.5388 - val_bag_output_acc: 0.5454 - val_footwear_output_acc: 0.6069 - val_pose_output_acc: 0.7157 - val_emotion_output_acc: 0.6316\n",
            "Epoch 25/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 4.1742 - gender_output_loss: 0.1381 - image_quality_output_loss: 0.5644 - age_output_loss: 0.9296 - weight_output_loss: 0.5835 - bag_output_loss: 0.4178 - footwear_output_loss: 0.4265 - pose_output_loss: 0.1973 - emotion_output_loss: 0.5016 - gender_output_acc: 0.9456 - image_quality_output_acc: 0.7558 - age_output_acc: 0.6207 - weight_output_acc: 0.7683 - bag_output_acc: 0.8299 - footwear_output_acc: 0.8239 - pose_output_acc: 0.9279 - emotion_output_acc: 0.8108 - val_loss: 10.9355 - val_gender_output_loss: 0.9480 - val_image_quality_output_loss: 1.3071 - val_age_output_loss: 1.7520 - val_weight_output_loss: 1.2823 - val_bag_output_loss: 1.4000 - val_footwear_output_loss: 1.2541 - val_pose_output_loss: 1.1635 - val_emotion_output_loss: 1.4132 - val_gender_output_acc: 0.7193 - val_image_quality_output_acc: 0.5242 - val_age_output_acc: 0.3231 - val_weight_output_acc: 0.5902 - val_bag_output_acc: 0.5121 - val_footwear_output_acc: 0.5736 - val_pose_output_acc: 0.7006 - val_emotion_output_acc: 0.6371\n",
            "Epoch 26/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 618ms/step - loss: 3.9220 - gender_output_loss: 0.1312 - image_quality_output_loss: 0.5374 - age_output_loss: 0.8688 - weight_output_loss: 0.5535 - bag_output_loss: 0.3880 - footwear_output_loss: 0.3971 - pose_output_loss: 0.1743 - emotion_output_loss: 0.4564 - gender_output_acc: 0.9483 - image_quality_output_acc: 0.7708 - age_output_acc: 0.6470 - weight_output_acc: 0.7783 - bag_output_acc: 0.8454 - footwear_output_acc: 0.8422 - pose_output_acc: 0.9350 - emotion_output_acc: 0.8261 - val_loss: 13.3288 - val_gender_output_loss: 1.2386 - val_image_quality_output_loss: 1.5013 - val_age_output_loss: 2.2218 - val_weight_output_loss: 1.5141 - val_bag_output_loss: 1.7274 - val_footwear_output_loss: 1.6650 - val_pose_output_loss: 1.3990 - val_emotion_output_loss: 1.6463 - val_gender_output_acc: 0.6774 - val_image_quality_output_acc: 0.4667 - val_age_output_acc: 0.3533 - val_weight_output_acc: 0.5212 - val_bag_output_acc: 0.5675 - val_footwear_output_acc: 0.5181 - val_pose_output_acc: 0.6689 - val_emotion_output_acc: 0.5852\n",
            "Epoch 27/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 623ms/step - loss: 3.6703 - gender_output_loss: 0.1286 - image_quality_output_loss: 0.4981 - age_output_loss: 0.8162 - weight_output_loss: 0.5090 - bag_output_loss: 0.3555 - footwear_output_loss: 0.3628 - pose_output_loss: 0.1554 - emotion_output_loss: 0.4295 - gender_output_acc: 0.9519 - image_quality_output_acc: 0.7891 - age_output_acc: 0.6780 - weight_output_acc: 0.8018 - bag_output_acc: 0.8576 - footwear_output_acc: 0.8563 - pose_output_acc: 0.9434 - emotion_output_acc: 0.8359 - val_loss: 10.9359 - val_gender_output_loss: 0.8528 - val_image_quality_output_loss: 1.3806 - val_age_output_loss: 1.8163 - val_weight_output_loss: 1.3379 - val_bag_output_loss: 1.3531 - val_footwear_output_loss: 1.2605 - val_pose_output_loss: 1.1144 - val_emotion_output_loss: 1.4052 - val_gender_output_acc: 0.7389 - val_image_quality_output_acc: 0.5096 - val_age_output_acc: 0.3337 - val_weight_output_acc: 0.5489 - val_bag_output_acc: 0.5459 - val_footwear_output_acc: 0.5751 - val_pose_output_acc: 0.7056 - val_emotion_output_acc: 0.5665\n",
            "Epoch 28/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 3.3736 - gender_output_loss: 0.0952 - image_quality_output_loss: 0.4580 - age_output_loss: 0.7554 - weight_output_loss: 0.4757 - bag_output_loss: 0.3206 - footwear_output_loss: 0.3332 - pose_output_loss: 0.1330 - emotion_output_loss: 0.3872 - gender_output_acc: 0.9641 - image_quality_output_acc: 0.8073 - age_output_acc: 0.7030 - weight_output_acc: 0.8156 - bag_output_acc: 0.8734 - footwear_output_acc: 0.8689 - pose_output_acc: 0.9531 - emotion_output_acc: 0.8574 - val_loss: 12.8181 - val_gender_output_loss: 1.0696 - val_image_quality_output_loss: 1.5754 - val_age_output_loss: 2.1721 - val_weight_output_loss: 1.4739 - val_bag_output_loss: 1.6333 - val_footwear_output_loss: 1.4589 - val_pose_output_loss: 1.3709 - val_emotion_output_loss: 1.6488 - val_gender_output_acc: 0.7490 - val_image_quality_output_acc: 0.5035 - val_age_output_acc: 0.3594 - val_weight_output_acc: 0.5539 - val_bag_output_acc: 0.5449 - val_footwear_output_acc: 0.5932 - val_pose_output_acc: 0.7082 - val_emotion_output_acc: 0.5907\n",
            "Epoch 29/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 3.1115 - gender_output_loss: 0.0856 - image_quality_output_loss: 0.4249 - age_output_loss: 0.6937 - weight_output_loss: 0.4251 - bag_output_loss: 0.2901 - footwear_output_loss: 0.3045 - pose_output_loss: 0.1251 - emotion_output_loss: 0.3474 - gender_output_acc: 0.9677 - image_quality_output_acc: 0.8227 - age_output_acc: 0.7251 - weight_output_acc: 0.8361 - bag_output_acc: 0.8872 - footwear_output_acc: 0.8775 - pose_output_acc: 0.9563 - emotion_output_acc: 0.8687 - val_loss: 14.2932 - val_gender_output_loss: 1.0955 - val_image_quality_output_loss: 1.8617 - val_age_output_loss: 2.3657 - val_weight_output_loss: 1.6317 - val_bag_output_loss: 2.1779 - val_footwear_output_loss: 1.6172 - val_pose_output_loss: 1.3496 - val_emotion_output_loss: 1.7785 - val_gender_output_acc: 0.7031 - val_image_quality_output_acc: 0.5060 - val_age_output_acc: 0.2984 - val_weight_output_acc: 0.5988 - val_bag_output_acc: 0.5670 - val_footwear_output_acc: 0.5796 - val_pose_output_acc: 0.6729 - val_emotion_output_acc: 0.6124\n",
            "Epoch 30/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 2.8104 - gender_output_loss: 0.0795 - image_quality_output_loss: 0.3841 - age_output_loss: 0.6446 - weight_output_loss: 0.3720 - bag_output_loss: 0.2443 - footwear_output_loss: 0.2664 - pose_output_loss: 0.1017 - emotion_output_loss: 0.3026 - gender_output_acc: 0.9692 - image_quality_output_acc: 0.8408 - age_output_acc: 0.7520 - weight_output_acc: 0.8585 - bag_output_acc: 0.9069 - footwear_output_acc: 0.8979 - pose_output_acc: 0.9626 - emotion_output_acc: 0.8895 - val_loss: 13.6462 - val_gender_output_loss: 1.1035 - val_image_quality_output_loss: 1.8029 - val_age_output_loss: 2.3480 - val_weight_output_loss: 1.5518 - val_bag_output_loss: 1.7752 - val_footwear_output_loss: 1.5154 - val_pose_output_loss: 1.3280 - val_emotion_output_loss: 1.8062 - val_gender_output_acc: 0.7409 - val_image_quality_output_acc: 0.4627 - val_age_output_acc: 0.3185 - val_weight_output_acc: 0.5504 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.5675 - val_pose_output_acc: 0.7107 - val_emotion_output_acc: 0.5806\n",
            "Epoch 31/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 2.5197 - gender_output_loss: 0.0728 - image_quality_output_loss: 0.3428 - age_output_loss: 0.5804 - weight_output_loss: 0.3244 - bag_output_loss: 0.2137 - footwear_output_loss: 0.2312 - pose_output_loss: 0.0754 - emotion_output_loss: 0.2637 - gender_output_acc: 0.9737 - image_quality_output_acc: 0.8608 - age_output_acc: 0.7832 - weight_output_acc: 0.8780 - bag_output_acc: 0.9185 - footwear_output_acc: 0.9125 - pose_output_acc: 0.9754 - emotion_output_acc: 0.9044 - val_loss: 13.8151 - val_gender_output_loss: 1.0873 - val_image_quality_output_loss: 1.7586 - val_age_output_loss: 2.5622 - val_weight_output_loss: 1.6543 - val_bag_output_loss: 1.7322 - val_footwear_output_loss: 1.4473 - val_pose_output_loss: 1.4541 - val_emotion_output_loss: 1.7039 - val_gender_output_acc: 0.7314 - val_image_quality_output_acc: 0.4607 - val_age_output_acc: 0.3448 - val_weight_output_acc: 0.5731 - val_bag_output_acc: 0.5418 - val_footwear_output_acc: 0.5786 - val_pose_output_acc: 0.6739 - val_emotion_output_acc: 0.5943\n",
            "Epoch 32/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 2.3141 - gender_output_loss: 0.0587 - image_quality_output_loss: 0.3178 - age_output_loss: 0.5217 - weight_output_loss: 0.2977 - bag_output_loss: 0.1954 - footwear_output_loss: 0.2015 - pose_output_loss: 0.0817 - emotion_output_loss: 0.2244 - gender_output_acc: 0.9811 - image_quality_output_acc: 0.8727 - age_output_acc: 0.8059 - weight_output_acc: 0.8874 - bag_output_acc: 0.9283 - footwear_output_acc: 0.9244 - pose_output_acc: 0.9726 - emotion_output_acc: 0.9213 - val_loss: 15.4430 - val_gender_output_loss: 1.2521 - val_image_quality_output_loss: 1.9024 - val_age_output_loss: 2.7170 - val_weight_output_loss: 1.8494 - val_bag_output_loss: 2.1186 - val_footwear_output_loss: 1.6533 - val_pose_output_loss: 1.4986 - val_emotion_output_loss: 2.0364 - val_gender_output_acc: 0.7329 - val_image_quality_output_acc: 0.4874 - val_age_output_acc: 0.3543 - val_weight_output_acc: 0.5776 - val_bag_output_acc: 0.5247 - val_footwear_output_acc: 0.5786 - val_pose_output_acc: 0.7107 - val_emotion_output_acc: 0.5373\n",
            "Epoch 33/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 2.1368 - gender_output_loss: 0.0657 - image_quality_output_loss: 0.2837 - age_output_loss: 0.4716 - weight_output_loss: 0.2665 - bag_output_loss: 0.1858 - footwear_output_loss: 0.1756 - pose_output_loss: 0.0702 - emotion_output_loss: 0.2025 - gender_output_acc: 0.9760 - image_quality_output_acc: 0.8905 - age_output_acc: 0.8196 - weight_output_acc: 0.9024 - bag_output_acc: 0.9296 - footwear_output_acc: 0.9367 - pose_output_acc: 0.9758 - emotion_output_acc: 0.9247 - val_loss: 14.3477 - val_gender_output_loss: 1.0672 - val_image_quality_output_loss: 1.7200 - val_age_output_loss: 2.5480 - val_weight_output_loss: 1.8575 - val_bag_output_loss: 1.8705 - val_footwear_output_loss: 1.6679 - val_pose_output_loss: 1.3610 - val_emotion_output_loss: 1.8403 - val_gender_output_acc: 0.7263 - val_image_quality_output_acc: 0.4824 - val_age_output_acc: 0.3226 - val_weight_output_acc: 0.6003 - val_bag_output_acc: 0.5514 - val_footwear_output_acc: 0.6154 - val_pose_output_acc: 0.7082 - val_emotion_output_acc: 0.5575\n",
            "Epoch 34/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 1.9226 - gender_output_loss: 0.0481 - image_quality_output_loss: 0.2463 - age_output_loss: 0.4325 - weight_output_loss: 0.2357 - bag_output_loss: 0.1463 - footwear_output_loss: 0.1659 - pose_output_loss: 0.0609 - emotion_output_loss: 0.1716 - gender_output_acc: 0.9845 - image_quality_output_acc: 0.9076 - age_output_acc: 0.8415 - weight_output_acc: 0.9141 - bag_output_acc: 0.9477 - footwear_output_acc: 0.9404 - pose_output_acc: 0.9798 - emotion_output_acc: 0.9404 - val_loss: 17.1263 - val_gender_output_loss: 1.2362 - val_image_quality_output_loss: 2.2126 - val_age_output_loss: 3.0020 - val_weight_output_loss: 2.1436 - val_bag_output_loss: 2.1961 - val_footwear_output_loss: 1.9746 - val_pose_output_loss: 1.7473 - val_emotion_output_loss: 2.1986 - val_gender_output_acc: 0.7288 - val_image_quality_output_acc: 0.4753 - val_age_output_acc: 0.2923 - val_weight_output_acc: 0.5050 - val_bag_output_acc: 0.5544 - val_footwear_output_acc: 0.5605 - val_pose_output_acc: 0.6971 - val_emotion_output_acc: 0.6053\n",
            "Epoch 35/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 621ms/step - loss: 1.6977 - gender_output_loss: 0.0364 - image_quality_output_loss: 0.2099 - age_output_loss: 0.3735 - weight_output_loss: 0.2090 - bag_output_loss: 0.1217 - footwear_output_loss: 0.1301 - pose_output_loss: 0.0576 - emotion_output_loss: 0.1442 - gender_output_acc: 0.9887 - image_quality_output_acc: 0.9214 - age_output_acc: 0.8666 - weight_output_acc: 0.9239 - bag_output_acc: 0.9574 - footwear_output_acc: 0.9546 - pose_output_acc: 0.9808 - emotion_output_acc: 0.9500 - val_loss: 16.2017 - val_gender_output_loss: 1.1968 - val_image_quality_output_loss: 2.2045 - val_age_output_loss: 2.7567 - val_weight_output_loss: 2.0938 - val_bag_output_loss: 2.0442 - val_footwear_output_loss: 1.8080 - val_pose_output_loss: 1.5273 - val_emotion_output_loss: 2.1551 - val_gender_output_acc: 0.7298 - val_image_quality_output_acc: 0.4587 - val_age_output_acc: 0.3311 - val_weight_output_acc: 0.5630 - val_bag_output_acc: 0.5575 - val_footwear_output_acc: 0.5827 - val_pose_output_acc: 0.7172 - val_emotion_output_acc: 0.5912\n",
            "Epoch 36/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 1.5587 - gender_output_loss: 0.0392 - image_quality_output_loss: 0.2043 - age_output_loss: 0.3322 - weight_output_loss: 0.1677 - bag_output_loss: 0.1034 - footwear_output_loss: 0.1237 - pose_output_loss: 0.0534 - emotion_output_loss: 0.1196 - gender_output_acc: 0.9876 - image_quality_output_acc: 0.9216 - age_output_acc: 0.8815 - weight_output_acc: 0.9405 - bag_output_acc: 0.9635 - footwear_output_acc: 0.9556 - pose_output_acc: 0.9816 - emotion_output_acc: 0.9616 - val_loss: 17.4692 - val_gender_output_loss: 1.2922 - val_image_quality_output_loss: 2.2236 - val_age_output_loss: 3.2522 - val_weight_output_loss: 2.1282 - val_bag_output_loss: 2.1916 - val_footwear_output_loss: 1.9985 - val_pose_output_loss: 1.6038 - val_emotion_output_loss: 2.3640 - val_gender_output_acc: 0.7329 - val_image_quality_output_acc: 0.4798 - val_age_output_acc: 0.3095 - val_weight_output_acc: 0.5640 - val_bag_output_acc: 0.5524 - val_footwear_output_acc: 0.5746 - val_pose_output_acc: 0.7137 - val_emotion_output_acc: 0.6099\n",
            "Epoch 37/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 1.3307 - gender_output_loss: 0.0259 - image_quality_output_loss: 0.1670 - age_output_loss: 0.2770 - weight_output_loss: 0.1321 - bag_output_loss: 0.0789 - footwear_output_loss: 0.1006 - pose_output_loss: 0.0391 - emotion_output_loss: 0.0950 - gender_output_acc: 0.9913 - image_quality_output_acc: 0.9413 - age_output_acc: 0.8968 - weight_output_acc: 0.9538 - bag_output_acc: 0.9750 - footwear_output_acc: 0.9661 - pose_output_acc: 0.9867 - emotion_output_acc: 0.9697 - val_loss: 17.7210 - val_gender_output_loss: 1.3029 - val_image_quality_output_loss: 2.4595 - val_age_output_loss: 3.2104 - val_weight_output_loss: 2.1882 - val_bag_output_loss: 2.2380 - val_footwear_output_loss: 2.0149 - val_pose_output_loss: 1.5962 - val_emotion_output_loss: 2.2959 - val_gender_output_acc: 0.7314 - val_image_quality_output_acc: 0.4607 - val_age_output_acc: 0.3024 - val_weight_output_acc: 0.5358 - val_bag_output_acc: 0.5408 - val_footwear_output_acc: 0.5948 - val_pose_output_acc: 0.7188 - val_emotion_output_acc: 0.5706\n",
            "Epoch 38/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 1.2182 - gender_output_loss: 0.0289 - image_quality_output_loss: 0.1301 - age_output_loss: 0.2374 - weight_output_loss: 0.1238 - bag_output_loss: 0.0718 - footwear_output_loss: 0.0821 - pose_output_loss: 0.0278 - emotion_output_loss: 0.1013 - gender_output_acc: 0.9916 - image_quality_output_acc: 0.9563 - age_output_acc: 0.9196 - weight_output_acc: 0.9580 - bag_output_acc: 0.9776 - footwear_output_acc: 0.9737 - pose_output_acc: 0.9919 - emotion_output_acc: 0.9669 - val_loss: 18.1560 - val_gender_output_loss: 1.3524 - val_image_quality_output_loss: 2.3880 - val_age_output_loss: 3.4077 - val_weight_output_loss: 2.2405 - val_bag_output_loss: 2.3099 - val_footwear_output_loss: 1.9788 - val_pose_output_loss: 1.6502 - val_emotion_output_loss: 2.4136 - val_gender_output_acc: 0.7213 - val_image_quality_output_acc: 0.4894 - val_age_output_acc: 0.3352 - val_weight_output_acc: 0.5680 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5721 - val_pose_output_acc: 0.6920 - val_emotion_output_acc: 0.5570\n",
            "Epoch 39/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 621ms/step - loss: 1.1861 - gender_output_loss: 0.0256 - image_quality_output_loss: 0.1295 - age_output_loss: 0.2161 - weight_output_loss: 0.1134 - bag_output_loss: 0.0842 - footwear_output_loss: 0.0770 - pose_output_loss: 0.0349 - emotion_output_loss: 0.0907 - gender_output_acc: 0.9916 - image_quality_output_acc: 0.9545 - age_output_acc: 0.9226 - weight_output_acc: 0.9612 - bag_output_acc: 0.9717 - footwear_output_acc: 0.9760 - pose_output_acc: 0.9887 - emotion_output_acc: 0.9701 - val_loss: 19.5675 - val_gender_output_loss: 1.3434 - val_image_quality_output_loss: 2.5671 - val_age_output_loss: 3.7984 - val_weight_output_loss: 2.5164 - val_bag_output_loss: 2.4823 - val_footwear_output_loss: 2.2064 - val_pose_output_loss: 1.7535 - val_emotion_output_loss: 2.4852 - val_gender_output_acc: 0.7364 - val_image_quality_output_acc: 0.4839 - val_age_output_acc: 0.3438 - val_weight_output_acc: 0.5817 - val_bag_output_acc: 0.5701 - val_footwear_output_acc: 0.5806 - val_pose_output_acc: 0.7162 - val_emotion_output_acc: 0.5262\n",
            "Epoch 40/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 1.0660 - gender_output_loss: 0.0190 - image_quality_output_loss: 0.1166 - age_output_loss: 0.1879 - weight_output_loss: 0.0837 - bag_output_loss: 0.0753 - footwear_output_loss: 0.0611 - pose_output_loss: 0.0360 - emotion_output_loss: 0.0718 - gender_output_acc: 0.9947 - image_quality_output_acc: 0.9583 - age_output_acc: 0.9360 - weight_output_acc: 0.9736 - bag_output_acc: 0.9751 - footwear_output_acc: 0.9813 - pose_output_acc: 0.9898 - emotion_output_acc: 0.9780 - val_loss: 19.4598 - val_gender_output_loss: 1.4421 - val_image_quality_output_loss: 2.5066 - val_age_output_loss: 3.7014 - val_weight_output_loss: 2.4059 - val_bag_output_loss: 2.5264 - val_footwear_output_loss: 2.1569 - val_pose_output_loss: 1.6871 - val_emotion_output_loss: 2.6189 - val_gender_output_acc: 0.7334 - val_image_quality_output_acc: 0.4662 - val_age_output_acc: 0.3337 - val_weight_output_acc: 0.5711 - val_bag_output_acc: 0.5519 - val_footwear_output_acc: 0.5917 - val_pose_output_acc: 0.7253 - val_emotion_output_acc: 0.6149\n",
            "Epoch 41/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 621ms/step - loss: 1.0319 - gender_output_loss: 0.0243 - image_quality_output_loss: 0.1091 - age_output_loss: 0.1782 - weight_output_loss: 0.0897 - bag_output_loss: 0.0661 - footwear_output_loss: 0.0524 - pose_output_loss: 0.0287 - emotion_output_loss: 0.0689 - gender_output_acc: 0.9923 - image_quality_output_acc: 0.9624 - age_output_acc: 0.9372 - weight_output_acc: 0.9714 - bag_output_acc: 0.9781 - footwear_output_acc: 0.9846 - pose_output_acc: 0.9905 - emotion_output_acc: 0.9780 - val_loss: 20.8443 - val_gender_output_loss: 1.3729 - val_image_quality_output_loss: 3.0000 - val_age_output_loss: 3.9261 - val_weight_output_loss: 2.6078 - val_bag_output_loss: 2.6592 - val_footwear_output_loss: 2.3500 - val_pose_output_loss: 1.7376 - val_emotion_output_loss: 2.7763 - val_gender_output_acc: 0.7460 - val_image_quality_output_acc: 0.4521 - val_age_output_acc: 0.3357 - val_weight_output_acc: 0.5529 - val_bag_output_acc: 0.5600 - val_footwear_output_acc: 0.5852 - val_pose_output_acc: 0.6935 - val_emotion_output_acc: 0.6270\n",
            "Epoch 42/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 1.0218 - gender_output_loss: 0.0249 - image_quality_output_loss: 0.1133 - age_output_loss: 0.1857 - weight_output_loss: 0.0793 - bag_output_loss: 0.0561 - footwear_output_loss: 0.0623 - pose_output_loss: 0.0345 - emotion_output_loss: 0.0514 - gender_output_acc: 0.9919 - image_quality_output_acc: 0.9626 - age_output_acc: 0.9352 - weight_output_acc: 0.9738 - bag_output_acc: 0.9819 - footwear_output_acc: 0.9808 - pose_output_acc: 0.9897 - emotion_output_acc: 0.9850 - val_loss: 21.3464 - val_gender_output_loss: 1.5183 - val_image_quality_output_loss: 2.9156 - val_age_output_loss: 3.9647 - val_weight_output_loss: 2.7876 - val_bag_output_loss: 2.6102 - val_footwear_output_loss: 2.4991 - val_pose_output_loss: 1.8444 - val_emotion_output_loss: 2.7922 - val_gender_output_acc: 0.7283 - val_image_quality_output_acc: 0.4758 - val_age_output_acc: 0.3352 - val_weight_output_acc: 0.5403 - val_bag_output_acc: 0.5650 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.6880 - val_emotion_output_acc: 0.5832\n",
            "Epoch 43/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 621ms/step - loss: 0.9823 - gender_output_loss: 0.0343 - image_quality_output_loss: 0.0994 - age_output_loss: 0.1480 - weight_output_loss: 0.0760 - bag_output_loss: 0.0688 - footwear_output_loss: 0.0556 - pose_output_loss: 0.0293 - emotion_output_loss: 0.0568 - gender_output_acc: 0.9880 - image_quality_output_acc: 0.9651 - age_output_acc: 0.9505 - weight_output_acc: 0.9756 - bag_output_acc: 0.9766 - footwear_output_acc: 0.9816 - pose_output_acc: 0.9910 - emotion_output_acc: 0.9837 - val_loss: 22.1225 - val_gender_output_loss: 1.4602 - val_image_quality_output_loss: 3.1116 - val_age_output_loss: 4.2452 - val_weight_output_loss: 2.8370 - val_bag_output_loss: 2.6572 - val_footwear_output_loss: 2.5630 - val_pose_output_loss: 1.9839 - val_emotion_output_loss: 2.8503 - val_gender_output_acc: 0.7409 - val_image_quality_output_acc: 0.4642 - val_age_output_acc: 0.3347 - val_weight_output_acc: 0.5222 - val_bag_output_acc: 0.5635 - val_footwear_output_acc: 0.6008 - val_pose_output_acc: 0.6956 - val_emotion_output_acc: 0.6048\n",
            "Epoch 44/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 0.8968 - gender_output_loss: 0.0269 - image_quality_output_loss: 0.0859 - age_output_loss: 0.1120 - weight_output_loss: 0.0787 - bag_output_loss: 0.0548 - footwear_output_loss: 0.0580 - pose_output_loss: 0.0284 - emotion_output_loss: 0.0381 - gender_output_acc: 0.9915 - image_quality_output_acc: 0.9714 - age_output_acc: 0.9647 - weight_output_acc: 0.9743 - bag_output_acc: 0.9809 - footwear_output_acc: 0.9812 - pose_output_acc: 0.9915 - emotion_output_acc: 0.9897 - val_loss: 20.8438 - val_gender_output_loss: 1.4173 - val_image_quality_output_loss: 2.8301 - val_age_output_loss: 4.0310 - val_weight_output_loss: 2.7203 - val_bag_output_loss: 2.4984 - val_footwear_output_loss: 2.4283 - val_pose_output_loss: 1.8122 - val_emotion_output_loss: 2.6924 - val_gender_output_acc: 0.7384 - val_image_quality_output_acc: 0.4990 - val_age_output_acc: 0.3175 - val_weight_output_acc: 0.5514 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5580 - val_pose_output_acc: 0.7319 - val_emotion_output_acc: 0.5948\n",
            "Epoch 45/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 623ms/step - loss: 0.8548 - gender_output_loss: 0.0219 - image_quality_output_loss: 0.0817 - age_output_loss: 0.1117 - weight_output_loss: 0.0572 - bag_output_loss: 0.0427 - footwear_output_loss: 0.0484 - pose_output_loss: 0.0277 - emotion_output_loss: 0.0498 - gender_output_acc: 0.9928 - image_quality_output_acc: 0.9727 - age_output_acc: 0.9649 - weight_output_acc: 0.9810 - bag_output_acc: 0.9870 - footwear_output_acc: 0.9856 - pose_output_acc: 0.9908 - emotion_output_acc: 0.9850 - val_loss: 22.4095 - val_gender_output_loss: 1.5219 - val_image_quality_output_loss: 3.0948 - val_age_output_loss: 4.4475 - val_weight_output_loss: 2.7895 - val_bag_output_loss: 2.8034 - val_footwear_output_loss: 2.5820 - val_pose_output_loss: 1.9136 - val_emotion_output_loss: 2.8432 - val_gender_output_acc: 0.7303 - val_image_quality_output_acc: 0.4803 - val_age_output_acc: 0.3105 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5136 - val_footwear_output_acc: 0.5524 - val_pose_output_acc: 0.7188 - val_emotion_output_acc: 0.5938\n",
            "Epoch 46/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 225s 625ms/step - loss: 0.7684 - gender_output_loss: 0.0126 - image_quality_output_loss: 0.0717 - age_output_loss: 0.1070 - weight_output_loss: 0.0416 - bag_output_loss: 0.0376 - footwear_output_loss: 0.0333 - pose_output_loss: 0.0163 - emotion_output_loss: 0.0348 - gender_output_acc: 0.9968 - image_quality_output_acc: 0.9752 - age_output_acc: 0.9663 - weight_output_acc: 0.9872 - bag_output_acc: 0.9886 - footwear_output_acc: 0.9902 - pose_output_acc: 0.9957 - emotion_output_acc: 0.9903 - val_loss: 22.0463 - val_gender_output_loss: 1.4250 - val_image_quality_output_loss: 3.1482 - val_age_output_loss: 4.1505 - val_weight_output_loss: 2.8069 - val_bag_output_loss: 2.8758 - val_footwear_output_loss: 2.4674 - val_pose_output_loss: 1.8905 - val_emotion_output_loss: 2.8686 - val_gender_output_acc: 0.7404 - val_image_quality_output_acc: 0.4526 - val_age_output_acc: 0.3236 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5726 - val_footwear_output_acc: 0.5761 - val_pose_output_acc: 0.6930 - val_emotion_output_acc: 0.6003\n",
            "Epoch 47/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 624ms/step - loss: 0.7507 - gender_output_loss: 0.0117 - image_quality_output_loss: 0.0563 - age_output_loss: 0.1034 - weight_output_loss: 0.0447 - bag_output_loss: 0.0488 - footwear_output_loss: 0.0242 - pose_output_loss: 0.0170 - emotion_output_loss: 0.0314 - gender_output_acc: 0.9972 - image_quality_output_acc: 0.9825 - age_output_acc: 0.9661 - weight_output_acc: 0.9877 - bag_output_acc: 0.9839 - footwear_output_acc: 0.9937 - pose_output_acc: 0.9948 - emotion_output_acc: 0.9914 - val_loss: 21.0531 - val_gender_output_loss: 1.3497 - val_image_quality_output_loss: 2.8631 - val_age_output_loss: 4.0725 - val_weight_output_loss: 2.8457 - val_bag_output_loss: 2.6274 - val_footwear_output_loss: 2.3795 - val_pose_output_loss: 1.6833 - val_emotion_output_loss: 2.8189 - val_gender_output_acc: 0.7470 - val_image_quality_output_acc: 0.4743 - val_age_output_acc: 0.3427 - val_weight_output_acc: 0.5932 - val_bag_output_acc: 0.5529 - val_footwear_output_acc: 0.5781 - val_pose_output_acc: 0.7258 - val_emotion_output_acc: 0.5524\n",
            "Epoch 48/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 225s 624ms/step - loss: 0.7636 - gender_output_loss: 0.0100 - image_quality_output_loss: 0.0522 - age_output_loss: 0.1204 - weight_output_loss: 0.0458 - bag_output_loss: 0.0448 - footwear_output_loss: 0.0264 - pose_output_loss: 0.0139 - emotion_output_loss: 0.0372 - gender_output_acc: 0.9973 - image_quality_output_acc: 0.9822 - age_output_acc: 0.9588 - weight_output_acc: 0.9860 - bag_output_acc: 0.9851 - footwear_output_acc: 0.9927 - pose_output_acc: 0.9960 - emotion_output_acc: 0.9883 - val_loss: 21.8664 - val_gender_output_loss: 1.4692 - val_image_quality_output_loss: 3.0151 - val_age_output_loss: 4.2878 - val_weight_output_loss: 2.9079 - val_bag_output_loss: 2.7116 - val_footwear_output_loss: 2.4543 - val_pose_output_loss: 1.7949 - val_emotion_output_loss: 2.8129 - val_gender_output_acc: 0.7434 - val_image_quality_output_acc: 0.4632 - val_age_output_acc: 0.3412 - val_weight_output_acc: 0.5847 - val_bag_output_acc: 0.5615 - val_footwear_output_acc: 0.6084 - val_pose_output_acc: 0.7243 - val_emotion_output_acc: 0.5877\n",
            "Epoch 49/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 225s 624ms/step - loss: 0.6454 - gender_output_loss: 0.0108 - image_quality_output_loss: 0.0397 - age_output_loss: 0.0777 - weight_output_loss: 0.0284 - bag_output_loss: 0.0220 - footwear_output_loss: 0.0207 - pose_output_loss: 0.0113 - emotion_output_loss: 0.0223 - gender_output_acc: 0.9970 - image_quality_output_acc: 0.9866 - age_output_acc: 0.9742 - weight_output_acc: 0.9918 - bag_output_acc: 0.9937 - footwear_output_acc: 0.9946 - pose_output_acc: 0.9967 - emotion_output_acc: 0.9946 - val_loss: 22.3977 - val_gender_output_loss: 1.4496 - val_image_quality_output_loss: 3.1003 - val_age_output_loss: 4.3410 - val_weight_output_loss: 3.0414 - val_bag_output_loss: 2.7371 - val_footwear_output_loss: 2.5808 - val_pose_output_loss: 1.8486 - val_emotion_output_loss: 2.8866 - val_gender_output_acc: 0.7399 - val_image_quality_output_acc: 0.4728 - val_age_output_acc: 0.3357 - val_weight_output_acc: 0.5842 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5822 - val_pose_output_acc: 0.7016 - val_emotion_output_acc: 0.6099\n",
            "Epoch 50/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 226s 627ms/step - loss: 0.6408 - gender_output_loss: 0.0075 - image_quality_output_loss: 0.0448 - age_output_loss: 0.0675 - weight_output_loss: 0.0314 - bag_output_loss: 0.0248 - footwear_output_loss: 0.0294 - pose_output_loss: 0.0089 - emotion_output_loss: 0.0143 - gender_output_acc: 0.9984 - image_quality_output_acc: 0.9865 - age_output_acc: 0.9789 - weight_output_acc: 0.9908 - bag_output_acc: 0.9924 - footwear_output_acc: 0.9914 - pose_output_acc: 0.9984 - emotion_output_acc: 0.9976 - val_loss: 23.1255 - val_gender_output_loss: 1.4839 - val_image_quality_output_loss: 3.4334 - val_age_output_loss: 4.5377 - val_weight_output_loss: 3.0147 - val_bag_output_loss: 2.8468 - val_footwear_output_loss: 2.5256 - val_pose_output_loss: 1.9483 - val_emotion_output_loss: 2.9231 - val_gender_output_acc: 0.7525 - val_image_quality_output_acc: 0.4869 - val_age_output_acc: 0.3155 - val_weight_output_acc: 0.5857 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5675 - val_pose_output_acc: 0.7253 - val_emotion_output_acc: 0.5922\n",
            "Epoch 51/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 0.6306 - gender_output_loss: 0.0065 - image_quality_output_loss: 0.0625 - age_output_loss: 0.0561 - weight_output_loss: 0.0264 - bag_output_loss: 0.0234 - footwear_output_loss: 0.0201 - pose_output_loss: 0.0089 - emotion_output_loss: 0.0149 - gender_output_acc: 0.9983 - image_quality_output_acc: 0.9791 - age_output_acc: 0.9832 - weight_output_acc: 0.9926 - bag_output_acc: 0.9938 - footwear_output_acc: 0.9953 - pose_output_acc: 0.9978 - emotion_output_acc: 0.9969 - val_loss: 22.2808 - val_gender_output_loss: 1.4497 - val_image_quality_output_loss: 3.1065 - val_age_output_loss: 4.4521 - val_weight_output_loss: 2.8353 - val_bag_output_loss: 2.6976 - val_footwear_output_loss: 2.6056 - val_pose_output_loss: 1.8148 - val_emotion_output_loss: 2.9076 - val_gender_output_acc: 0.7308 - val_image_quality_output_acc: 0.4839 - val_age_output_acc: 0.3352 - val_weight_output_acc: 0.5252 - val_bag_output_acc: 0.5559 - val_footwear_output_acc: 0.5968 - val_pose_output_acc: 0.7072 - val_emotion_output_acc: 0.5680\n",
            "Epoch 52/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 621ms/step - loss: 0.5886 - gender_output_loss: 0.0068 - image_quality_output_loss: 0.0316 - age_output_loss: 0.0559 - weight_output_loss: 0.0196 - bag_output_loss: 0.0189 - footwear_output_loss: 0.0229 - pose_output_loss: 0.0067 - emotion_output_loss: 0.0147 - gender_output_acc: 0.9983 - image_quality_output_acc: 0.9906 - age_output_acc: 0.9831 - weight_output_acc: 0.9941 - bag_output_acc: 0.9952 - footwear_output_acc: 0.9932 - pose_output_acc: 0.9984 - emotion_output_acc: 0.9970 - val_loss: 24.5140 - val_gender_output_loss: 1.6056 - val_image_quality_output_loss: 3.3731 - val_age_output_loss: 4.8533 - val_weight_output_loss: 3.2268 - val_bag_output_loss: 3.0057 - val_footwear_output_loss: 2.8765 - val_pose_output_loss: 1.9995 - val_emotion_output_loss: 3.1623 - val_gender_output_acc: 0.7460 - val_image_quality_output_acc: 0.4894 - val_age_output_acc: 0.3256 - val_weight_output_acc: 0.5655 - val_bag_output_acc: 0.5459 - val_footwear_output_acc: 0.5605 - val_pose_output_acc: 0.7203 - val_emotion_output_acc: 0.6149\n",
            "Epoch 53/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 0.5399 - gender_output_loss: 0.0066 - image_quality_output_loss: 0.0233 - age_output_loss: 0.0429 - weight_output_loss: 0.0152 - bag_output_loss: 0.0127 - footwear_output_loss: 0.0147 - pose_output_loss: 0.0064 - emotion_output_loss: 0.0071 - gender_output_acc: 0.9979 - image_quality_output_acc: 0.9937 - age_output_acc: 0.9878 - weight_output_acc: 0.9970 - bag_output_acc: 0.9967 - footwear_output_acc: 0.9962 - pose_output_acc: 0.9983 - emotion_output_acc: 0.9990 - val_loss: 24.0374 - val_gender_output_loss: 1.4834 - val_image_quality_output_loss: 3.5090 - val_age_output_loss: 4.8060 - val_weight_output_loss: 3.1334 - val_bag_output_loss: 2.8752 - val_footwear_output_loss: 2.7195 - val_pose_output_loss: 1.9223 - val_emotion_output_loss: 3.1778 - val_gender_output_acc: 0.7399 - val_image_quality_output_acc: 0.4798 - val_age_output_acc: 0.3206 - val_weight_output_acc: 0.5403 - val_bag_output_acc: 0.5454 - val_footwear_output_acc: 0.5882 - val_pose_output_acc: 0.7152 - val_emotion_output_acc: 0.6129\n",
            "Epoch 54/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 0.5207 - gender_output_loss: 0.0023 - image_quality_output_loss: 0.0315 - age_output_loss: 0.0297 - weight_output_loss: 0.0112 - bag_output_loss: 0.0105 - footwear_output_loss: 0.0102 - pose_output_loss: 0.0035 - emotion_output_loss: 0.0113 - gender_output_acc: 0.9997 - image_quality_output_acc: 0.9905 - age_output_acc: 0.9925 - weight_output_acc: 0.9977 - bag_output_acc: 0.9975 - footwear_output_acc: 0.9977 - pose_output_acc: 0.9993 - emotion_output_acc: 0.9976 - val_loss: 24.8137 - val_gender_output_loss: 1.6340 - val_image_quality_output_loss: 3.5560 - val_age_output_loss: 4.9798 - val_weight_output_loss: 3.1917 - val_bag_output_loss: 3.0822 - val_footwear_output_loss: 2.8389 - val_pose_output_loss: 1.9641 - val_emotion_output_loss: 3.1567 - val_gender_output_acc: 0.7409 - val_image_quality_output_acc: 0.4718 - val_age_output_acc: 0.3337 - val_weight_output_acc: 0.5706 - val_bag_output_acc: 0.5791 - val_footwear_output_acc: 0.5811 - val_pose_output_acc: 0.7344 - val_emotion_output_acc: 0.5862\n",
            "Epoch 55/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 0.5208 - gender_output_loss: 0.0026 - image_quality_output_loss: 0.0292 - age_output_loss: 0.0368 - weight_output_loss: 0.0049 - bag_output_loss: 0.0069 - footwear_output_loss: 0.0164 - pose_output_loss: 0.0049 - emotion_output_loss: 0.0090 - gender_output_acc: 0.9994 - image_quality_output_acc: 0.9909 - age_output_acc: 0.9898 - weight_output_acc: 1.0000 - bag_output_acc: 0.9987 - footwear_output_acc: 0.9956 - pose_output_acc: 0.9988 - emotion_output_acc: 0.9983 - val_loss: 24.2522 - val_gender_output_loss: 1.5635 - val_image_quality_output_loss: 3.3624 - val_age_output_loss: 4.8849 - val_weight_output_loss: 3.1046 - val_bag_output_loss: 2.9512 - val_footwear_output_loss: 2.8529 - val_pose_output_loss: 1.9953 - val_emotion_output_loss: 3.1277 - val_gender_output_acc: 0.7465 - val_image_quality_output_acc: 0.4798 - val_age_output_acc: 0.3185 - val_weight_output_acc: 0.5625 - val_bag_output_acc: 0.5504 - val_footwear_output_acc: 0.5811 - val_pose_output_acc: 0.7203 - val_emotion_output_acc: 0.5917\n",
            "Epoch 56/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 621ms/step - loss: 0.5302 - gender_output_loss: 0.0048 - image_quality_output_loss: 0.0298 - age_output_loss: 0.0400 - weight_output_loss: 0.0074 - bag_output_loss: 0.0082 - footwear_output_loss: 0.0101 - pose_output_loss: 0.0073 - emotion_output_loss: 0.0131 - gender_output_acc: 0.9987 - image_quality_output_acc: 0.9912 - age_output_acc: 0.9878 - weight_output_acc: 0.9988 - bag_output_acc: 0.9983 - footwear_output_acc: 0.9978 - pose_output_acc: 0.9977 - emotion_output_acc: 0.9967 - val_loss: 24.4657 - val_gender_output_loss: 1.6098 - val_image_quality_output_loss: 3.4446 - val_age_output_loss: 4.8757 - val_weight_output_loss: 3.1901 - val_bag_output_loss: 2.9946 - val_footwear_output_loss: 2.7443 - val_pose_output_loss: 2.0442 - val_emotion_output_loss: 3.1531 - val_gender_output_acc: 0.7470 - val_image_quality_output_acc: 0.4561 - val_age_output_acc: 0.3271 - val_weight_output_acc: 0.5771 - val_bag_output_acc: 0.5605 - val_footwear_output_acc: 0.5811 - val_pose_output_acc: 0.7213 - val_emotion_output_acc: 0.6069\n",
            "Epoch 57/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 621ms/step - loss: 0.5061 - gender_output_loss: 0.0066 - image_quality_output_loss: 0.0167 - age_output_loss: 0.0342 - weight_output_loss: 0.0109 - bag_output_loss: 0.0085 - footwear_output_loss: 0.0072 - pose_output_loss: 0.0043 - emotion_output_loss: 0.0087 - gender_output_acc: 0.9979 - image_quality_output_acc: 0.9952 - age_output_acc: 0.9904 - weight_output_acc: 0.9975 - bag_output_acc: 0.9977 - footwear_output_acc: 0.9988 - pose_output_acc: 0.9990 - emotion_output_acc: 0.9982 - val_loss: 24.5282 - val_gender_output_loss: 1.6686 - val_image_quality_output_loss: 3.3645 - val_age_output_loss: 4.9560 - val_weight_output_loss: 3.1759 - val_bag_output_loss: 3.0210 - val_footwear_output_loss: 2.7636 - val_pose_output_loss: 1.9914 - val_emotion_output_loss: 3.1785 - val_gender_output_acc: 0.7349 - val_image_quality_output_acc: 0.4849 - val_age_output_acc: 0.3372 - val_weight_output_acc: 0.5887 - val_bag_output_acc: 0.5378 - val_footwear_output_acc: 0.5766 - val_pose_output_acc: 0.7308 - val_emotion_output_acc: 0.6079\n",
            "Epoch 58/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 0.4927 - gender_output_loss: 0.0041 - image_quality_output_loss: 0.0137 - age_output_loss: 0.0314 - weight_output_loss: 0.0147 - bag_output_loss: 0.0069 - footwear_output_loss: 0.0064 - pose_output_loss: 0.0018 - emotion_output_loss: 0.0051 - gender_output_acc: 0.9990 - image_quality_output_acc: 0.9969 - age_output_acc: 0.9911 - weight_output_acc: 0.9964 - bag_output_acc: 0.9989 - footwear_output_acc: 0.9990 - pose_output_acc: 0.9998 - emotion_output_acc: 0.9994 - val_loss: 24.6551 - val_gender_output_loss: 1.5223 - val_image_quality_output_loss: 3.6207 - val_age_output_loss: 4.9372 - val_weight_output_loss: 3.2172 - val_bag_output_loss: 3.0073 - val_footwear_output_loss: 2.7957 - val_pose_output_loss: 1.9989 - val_emotion_output_loss: 3.1474 - val_gender_output_acc: 0.7470 - val_image_quality_output_acc: 0.4980 - val_age_output_acc: 0.3160 - val_weight_output_acc: 0.5338 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5711 - val_pose_output_acc: 0.7218 - val_emotion_output_acc: 0.5781\n",
            "Epoch 59/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 224s 621ms/step - loss: 0.4826 - gender_output_loss: 0.0026 - image_quality_output_loss: 0.0122 - age_output_loss: 0.0291 - weight_output_loss: 0.0127 - bag_output_loss: 0.0040 - footwear_output_loss: 0.0078 - pose_output_loss: 0.0018 - emotion_output_loss: 0.0043 - gender_output_acc: 0.9996 - image_quality_output_acc: 0.9965 - age_output_acc: 0.9922 - weight_output_acc: 0.9965 - bag_output_acc: 0.9993 - footwear_output_acc: 0.9977 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9995 - val_loss: 25.3606 - val_gender_output_loss: 1.5601 - val_image_quality_output_loss: 3.6449 - val_age_output_loss: 5.0177 - val_weight_output_loss: 3.2961 - val_bag_output_loss: 3.0838 - val_footwear_output_loss: 3.0179 - val_pose_output_loss: 2.1173 - val_emotion_output_loss: 3.2149 - val_gender_output_acc: 0.7434 - val_image_quality_output_acc: 0.4803 - val_age_output_acc: 0.3196 - val_weight_output_acc: 0.5731 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.5837 - val_pose_output_acc: 0.7238 - val_emotion_output_acc: 0.5963\n",
            "Epoch 60/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 0.4887 - gender_output_loss: 0.0037 - image_quality_output_loss: 0.0163 - age_output_loss: 0.0348 - weight_output_loss: 0.0083 - bag_output_loss: 0.0043 - footwear_output_loss: 0.0069 - pose_output_loss: 0.0025 - emotion_output_loss: 0.0043 - gender_output_acc: 0.9990 - image_quality_output_acc: 0.9954 - age_output_acc: 0.9898 - weight_output_acc: 0.9986 - bag_output_acc: 0.9992 - footwear_output_acc: 0.9984 - pose_output_acc: 0.9993 - emotion_output_acc: 0.9994 - val_loss: 25.8295 - val_gender_output_loss: 1.6685 - val_image_quality_output_loss: 3.7207 - val_age_output_loss: 5.1517 - val_weight_output_loss: 3.3453 - val_bag_output_loss: 3.1469 - val_footwear_output_loss: 2.9604 - val_pose_output_loss: 2.1560 - val_emotion_output_loss: 3.2726 - val_gender_output_acc: 0.7349 - val_image_quality_output_acc: 0.4819 - val_age_output_acc: 0.3155 - val_weight_output_acc: 0.5837 - val_bag_output_acc: 0.5600 - val_footwear_output_acc: 0.5958 - val_pose_output_acc: 0.7177 - val_emotion_output_acc: 0.6033\n",
            "Epoch 61/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 0.5277 - gender_output_loss: 0.0039 - image_quality_output_loss: 0.0282 - age_output_loss: 0.0453 - weight_output_loss: 0.0107 - bag_output_loss: 0.0057 - footwear_output_loss: 0.0109 - pose_output_loss: 0.0076 - emotion_output_loss: 0.0084 - gender_output_acc: 0.9992 - image_quality_output_acc: 0.9908 - age_output_acc: 0.9853 - weight_output_acc: 0.9978 - bag_output_acc: 0.9990 - footwear_output_acc: 0.9972 - pose_output_acc: 0.9977 - emotion_output_acc: 0.9976 - val_loss: 26.0735 - val_gender_output_loss: 1.6448 - val_image_quality_output_loss: 3.8166 - val_age_output_loss: 5.1095 - val_weight_output_loss: 3.3984 - val_bag_output_loss: 3.2673 - val_footwear_output_loss: 3.0099 - val_pose_output_loss: 2.1434 - val_emotion_output_loss: 3.2767 - val_gender_output_acc: 0.7480 - val_image_quality_output_acc: 0.4839 - val_age_output_acc: 0.3317 - val_weight_output_acc: 0.5464 - val_bag_output_acc: 0.5751 - val_footwear_output_acc: 0.5837 - val_pose_output_acc: 0.7162 - val_emotion_output_acc: 0.5922\n",
            "Epoch 62/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 0.5569 - gender_output_loss: 0.0048 - image_quality_output_loss: 0.0257 - age_output_loss: 0.0578 - weight_output_loss: 0.0237 - bag_output_loss: 0.0067 - footwear_output_loss: 0.0136 - pose_output_loss: 0.0076 - emotion_output_loss: 0.0104 - gender_output_acc: 0.9987 - image_quality_output_acc: 0.9918 - age_output_acc: 0.9810 - weight_output_acc: 0.9924 - bag_output_acc: 0.9985 - footwear_output_acc: 0.9964 - pose_output_acc: 0.9977 - emotion_output_acc: 0.9975 - val_loss: 24.0236 - val_gender_output_loss: 1.5067 - val_image_quality_output_loss: 3.3999 - val_age_output_loss: 4.7627 - val_weight_output_loss: 3.1282 - val_bag_output_loss: 2.9724 - val_footwear_output_loss: 2.6852 - val_pose_output_loss: 1.9784 - val_emotion_output_loss: 3.1838 - val_gender_output_acc: 0.7409 - val_image_quality_output_acc: 0.4693 - val_age_output_acc: 0.3039 - val_weight_output_acc: 0.5101 - val_bag_output_acc: 0.5570 - val_footwear_output_acc: 0.6003 - val_pose_output_acc: 0.7031 - val_emotion_output_acc: 0.5902\n",
            "Epoch 63/200\n",
            "Learning rate:  0.001\n",
            " 97/360 [=======>......................] - ETA: 2:35 - loss: 0.5385 - gender_output_loss: 0.0069 - image_quality_output_loss: 0.0232 - age_output_loss: 0.0475 - weight_output_loss: 0.0186 - bag_output_loss: 0.0043 - footwear_output_loss: 0.0143 - pose_output_loss: 0.0049 - emotion_output_loss: 0.0125 - gender_output_acc: 0.9977 - image_quality_output_acc: 0.9936 - age_output_acc: 0.9849 - weight_output_acc: 0.9942 - bag_output_acc: 0.9997 - footwear_output_acc: 0.9955 - pose_output_acc: 0.9984 - emotion_output_acc: 0.9974"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKuSpaPGp4zI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('gdrive/My Drive/Assignment5/model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuIkkT3vqR82",
        "colab_type": "code",
        "outputId": "0a289965-affa-4851-f8a8-775197ce3fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6,\n",
        "    initial_epoch=63, \n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 64/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 237s 659ms/step - loss: 4.5505 - gender_output_loss: 0.2574 - image_quality_output_loss: 0.5967 - age_output_loss: 0.8238 - weight_output_loss: 0.5544 - bag_output_loss: 0.5262 - footwear_output_loss: 0.5080 - pose_output_loss: 0.3370 - emotion_output_loss: 0.5381 - gender_output_acc: 0.9187 - image_quality_output_acc: 0.8214 - age_output_acc: 0.7655 - weight_output_acc: 0.8414 - bag_output_acc: 0.8457 - footwear_output_acc: 0.8551 - pose_output_acc: 0.9044 - emotion_output_acc: 0.8530 - val_loss: 5.3001 - val_gender_output_loss: 0.3098 - val_image_quality_output_loss: 0.7186 - val_age_output_loss: 0.9800 - val_weight_output_loss: 0.5840 - val_bag_output_loss: 0.6496 - val_footwear_output_loss: 0.6550 - val_pose_output_loss: 0.4337 - val_emotion_output_loss: 0.5601 - val_gender_output_acc: 0.8846 - val_image_quality_output_acc: 0.7434 - val_age_output_acc: 0.6956 - val_weight_output_acc: 0.7944 - val_bag_output_acc: 0.7949 - val_footwear_output_acc: 0.7833 - val_pose_output_acc: 0.8533 - val_emotion_output_acc: 0.8256\n",
            "Epoch 65/200\n",
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "360/360 [==============================] - 221s 614ms/step - loss: 2.5386 - gender_output_loss: 0.1157 - image_quality_output_loss: 0.2991 - age_output_loss: 0.4567 - weight_output_loss: 0.2890 - bag_output_loss: 0.2769 - footwear_output_loss: 0.2532 - pose_output_loss: 0.1548 - emotion_output_loss: 0.2839 - gender_output_acc: 0.9608 - image_quality_output_acc: 0.9010 - age_output_acc: 0.8576 - weight_output_acc: 0.9133 - bag_output_acc: 0.9082 - footwear_output_acc: 0.9201 - pose_output_acc: 0.9524 - emotion_output_acc: 0.9208 - val_loss: 4.2336 - val_gender_output_loss: 0.2413 - val_image_quality_output_loss: 0.4929 - val_age_output_loss: 0.8532 - val_weight_output_loss: 0.4995 - val_bag_output_loss: 0.4829 - val_footwear_output_loss: 0.4688 - val_pose_output_loss: 0.3234 - val_emotion_output_loss: 0.4624 - val_gender_output_acc: 0.9279 - val_image_quality_output_acc: 0.8342 - val_age_output_acc: 0.7666 - val_weight_output_acc: 0.8553 - val_bag_output_acc: 0.8488 - val_footwear_output_acc: 0.8579 - val_pose_output_acc: 0.9148 - val_emotion_output_acc: 0.8690\n",
            "Epoch 66/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 1.7145 - gender_output_loss: 0.0641 - image_quality_output_loss: 0.1850 - age_output_loss: 0.3174 - weight_output_loss: 0.1665 - bag_output_loss: 0.1591 - footwear_output_loss: 0.1602 - pose_output_loss: 0.0809 - emotion_output_loss: 0.1721 - gender_output_acc: 0.9798 - image_quality_output_acc: 0.9387 - age_output_acc: 0.8973 - weight_output_acc: 0.9493 - bag_output_acc: 0.9467 - footwear_output_acc: 0.9485 - pose_output_acc: 0.9764 - emotion_output_acc: 0.9507 - val_loss: 4.1065 - val_gender_output_loss: 0.2764 - val_image_quality_output_loss: 0.4354 - val_age_output_loss: 0.8272 - val_weight_output_loss: 0.4976 - val_bag_output_loss: 0.4286 - val_footwear_output_loss: 0.4883 - val_pose_output_loss: 0.2690 - val_emotion_output_loss: 0.4751 - val_gender_output_acc: 0.9078 - val_image_quality_output_acc: 0.8710 - val_age_output_acc: 0.7626 - val_weight_output_acc: 0.8453 - val_bag_output_acc: 0.8669 - val_footwear_output_acc: 0.8624 - val_pose_output_acc: 0.9214 - val_emotion_output_acc: 0.8775\n",
            "Epoch 67/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 1.1998 - gender_output_loss: 0.0384 - image_quality_output_loss: 0.1081 - age_output_loss: 0.2064 - weight_output_loss: 0.1006 - bag_output_loss: 0.0958 - footwear_output_loss: 0.0939 - pose_output_loss: 0.0451 - emotion_output_loss: 0.1026 - gender_output_acc: 0.9868 - image_quality_output_acc: 0.9652 - age_output_acc: 0.9332 - weight_output_acc: 0.9692 - bag_output_acc: 0.9703 - footwear_output_acc: 0.9707 - pose_output_acc: 0.9885 - emotion_output_acc: 0.9697 - val_loss: 4.4787 - val_gender_output_loss: 0.2478 - val_image_quality_output_loss: 0.5880 - val_age_output_loss: 0.8826 - val_weight_output_loss: 0.5881 - val_bag_output_loss: 0.4733 - val_footwear_output_loss: 0.5316 - val_pose_output_loss: 0.2738 - val_emotion_output_loss: 0.4848 - val_gender_output_acc: 0.9194 - val_image_quality_output_acc: 0.8276 - val_age_output_acc: 0.7702 - val_weight_output_acc: 0.8347 - val_bag_output_acc: 0.8599 - val_footwear_output_acc: 0.8553 - val_pose_output_acc: 0.9279 - val_emotion_output_acc: 0.8725\n",
            "Epoch 68/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.8797 - gender_output_loss: 0.0185 - image_quality_output_loss: 0.0694 - age_output_loss: 0.1293 - weight_output_loss: 0.0625 - bag_output_loss: 0.0538 - footwear_output_loss: 0.0515 - pose_output_loss: 0.0275 - emotion_output_loss: 0.0586 - gender_output_acc: 0.9951 - image_quality_output_acc: 0.9776 - age_output_acc: 0.9603 - weight_output_acc: 0.9810 - bag_output_acc: 0.9843 - footwear_output_acc: 0.9852 - pose_output_acc: 0.9944 - emotion_output_acc: 0.9839 - val_loss: 4.2517 - val_gender_output_loss: 0.2274 - val_image_quality_output_loss: 0.5137 - val_age_output_loss: 0.8919 - val_weight_output_loss: 0.4981 - val_bag_output_loss: 0.4887 - val_footwear_output_loss: 0.4754 - val_pose_output_loss: 0.3145 - val_emotion_output_loss: 0.4337 - val_gender_output_acc: 0.9430 - val_image_quality_output_acc: 0.8679 - val_age_output_acc: 0.7707 - val_weight_output_acc: 0.8780 - val_bag_output_acc: 0.8715 - val_footwear_output_acc: 0.8926 - val_pose_output_acc: 0.9264 - val_emotion_output_acc: 0.8926\n",
            "Epoch 69/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.7259 - gender_output_loss: 0.0120 - image_quality_output_loss: 0.0497 - age_output_loss: 0.0871 - weight_output_loss: 0.0352 - bag_output_loss: 0.0439 - footwear_output_loss: 0.0338 - pose_output_loss: 0.0183 - emotion_output_loss: 0.0378 - gender_output_acc: 0.9969 - image_quality_output_acc: 0.9853 - age_output_acc: 0.9757 - weight_output_acc: 0.9914 - bag_output_acc: 0.9867 - footwear_output_acc: 0.9918 - pose_output_acc: 0.9962 - emotion_output_acc: 0.9916 - val_loss: 4.3453 - val_gender_output_loss: 0.2471 - val_image_quality_output_loss: 0.4612 - val_age_output_loss: 0.8937 - val_weight_output_loss: 0.5317 - val_bag_output_loss: 0.4921 - val_footwear_output_loss: 0.5103 - val_pose_output_loss: 0.3130 - val_emotion_output_loss: 0.4884 - val_gender_output_acc: 0.9425 - val_image_quality_output_acc: 0.8826 - val_age_output_acc: 0.7893 - val_weight_output_acc: 0.8765 - val_bag_output_acc: 0.8826 - val_footwear_output_acc: 0.8906 - val_pose_output_acc: 0.9264 - val_emotion_output_acc: 0.8942\n",
            "Epoch 70/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.5928 - gender_output_loss: 0.0058 - image_quality_output_loss: 0.0255 - age_output_loss: 0.0551 - weight_output_loss: 0.0194 - bag_output_loss: 0.0254 - footwear_output_loss: 0.0203 - pose_output_loss: 0.0126 - emotion_output_loss: 0.0211 - gender_output_acc: 0.9989 - image_quality_output_acc: 0.9942 - age_output_acc: 0.9875 - weight_output_acc: 0.9971 - bag_output_acc: 0.9933 - footwear_output_acc: 0.9960 - pose_output_acc: 0.9983 - emotion_output_acc: 0.9956 - val_loss: 4.5392 - val_gender_output_loss: 0.2207 - val_image_quality_output_loss: 0.5668 - val_age_output_loss: 0.9288 - val_weight_output_loss: 0.4948 - val_bag_output_loss: 0.5370 - val_footwear_output_loss: 0.5734 - val_pose_output_loss: 0.3223 - val_emotion_output_loss: 0.4880 - val_gender_output_acc: 0.9481 - val_image_quality_output_acc: 0.8735 - val_age_output_acc: 0.8075 - val_weight_output_acc: 0.9012 - val_bag_output_acc: 0.8831 - val_footwear_output_acc: 0.8800 - val_pose_output_acc: 0.9410 - val_emotion_output_acc: 0.8957\n",
            "Epoch 71/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 221s 613ms/step - loss: 0.5361 - gender_output_loss: 0.0060 - image_quality_output_loss: 0.0183 - age_output_loss: 0.0381 - weight_output_loss: 0.0141 - bag_output_loss: 0.0191 - footwear_output_loss: 0.0123 - pose_output_loss: 0.0093 - emotion_output_loss: 0.0118 - gender_output_acc: 0.9983 - image_quality_output_acc: 0.9962 - age_output_acc: 0.9923 - weight_output_acc: 0.9979 - bag_output_acc: 0.9956 - footwear_output_acc: 0.9986 - pose_output_acc: 0.9990 - emotion_output_acc: 0.9990 - val_loss: 4.1109 - val_gender_output_loss: 0.2098 - val_image_quality_output_loss: 0.4665 - val_age_output_loss: 0.8760 - val_weight_output_loss: 0.4552 - val_bag_output_loss: 0.4104 - val_footwear_output_loss: 0.5266 - val_pose_output_loss: 0.3115 - val_emotion_output_loss: 0.4480 - val_gender_output_acc: 0.9546 - val_image_quality_output_acc: 0.8926 - val_age_output_acc: 0.8236 - val_weight_output_acc: 0.9168 - val_bag_output_acc: 0.9234 - val_footwear_output_acc: 0.9022 - val_pose_output_acc: 0.9425 - val_emotion_output_acc: 0.9189\n",
            "Epoch 72/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4930 - gender_output_loss: 0.0037 - image_quality_output_loss: 0.0123 - age_output_loss: 0.0270 - weight_output_loss: 0.0078 - bag_output_loss: 0.0107 - footwear_output_loss: 0.0092 - pose_output_loss: 0.0078 - emotion_output_loss: 0.0080 - gender_output_acc: 0.9994 - image_quality_output_acc: 0.9979 - age_output_acc: 0.9961 - weight_output_acc: 0.9994 - bag_output_acc: 0.9984 - footwear_output_acc: 0.9996 - pose_output_acc: 0.9993 - emotion_output_acc: 0.9997 - val_loss: 4.1625 - val_gender_output_loss: 0.1954 - val_image_quality_output_loss: 0.4828 - val_age_output_loss: 0.8920 - val_weight_output_loss: 0.5023 - val_bag_output_loss: 0.4772 - val_footwear_output_loss: 0.5001 - val_pose_output_loss: 0.2712 - val_emotion_output_loss: 0.4353 - val_gender_output_acc: 0.9587 - val_image_quality_output_acc: 0.8967 - val_age_output_acc: 0.8311 - val_weight_output_acc: 0.9118 - val_bag_output_acc: 0.9027 - val_footwear_output_acc: 0.9007 - val_pose_output_acc: 0.9496 - val_emotion_output_acc: 0.9214\n",
            "Epoch 73/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4679 - gender_output_loss: 0.0023 - image_quality_output_loss: 0.0095 - age_output_loss: 0.0178 - weight_output_loss: 0.0049 - bag_output_loss: 0.0077 - footwear_output_loss: 0.0070 - pose_output_loss: 0.0064 - emotion_output_loss: 0.0063 - gender_output_acc: 0.9997 - image_quality_output_acc: 0.9982 - age_output_acc: 0.9986 - weight_output_acc: 0.9999 - bag_output_acc: 0.9991 - footwear_output_acc: 0.9994 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9997 - val_loss: 4.0133 - val_gender_output_loss: 0.1801 - val_image_quality_output_loss: 0.4471 - val_age_output_loss: 0.8650 - val_weight_output_loss: 0.4774 - val_bag_output_loss: 0.4217 - val_footwear_output_loss: 0.5074 - val_pose_output_loss: 0.2829 - val_emotion_output_loss: 0.4261 - val_gender_output_acc: 0.9612 - val_image_quality_output_acc: 0.9098 - val_age_output_acc: 0.8448 - val_weight_output_acc: 0.9199 - val_bag_output_acc: 0.9168 - val_footwear_output_acc: 0.9133 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9299\n",
            "Epoch 74/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4505 - gender_output_loss: 0.0014 - image_quality_output_loss: 0.0053 - age_output_loss: 0.0132 - weight_output_loss: 0.0032 - bag_output_loss: 0.0057 - footwear_output_loss: 0.0058 - pose_output_loss: 0.0055 - emotion_output_loss: 0.0050 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9997 - age_output_acc: 0.9997 - weight_output_acc: 0.9999 - bag_output_acc: 0.9995 - footwear_output_acc: 0.9997 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9997 - val_loss: 4.0453 - val_gender_output_loss: 0.1854 - val_image_quality_output_loss: 0.4588 - val_age_output_loss: 0.8567 - val_weight_output_loss: 0.4714 - val_bag_output_loss: 0.4485 - val_footwear_output_loss: 0.5050 - val_pose_output_loss: 0.2904 - val_emotion_output_loss: 0.4240 - val_gender_output_acc: 0.9587 - val_image_quality_output_acc: 0.9123 - val_age_output_acc: 0.8498 - val_weight_output_acc: 0.9158 - val_bag_output_acc: 0.9108 - val_footwear_output_acc: 0.9173 - val_pose_output_acc: 0.9506 - val_emotion_output_acc: 0.9264\n",
            "Epoch 75/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4469 - gender_output_loss: 0.0014 - image_quality_output_loss: 0.0051 - age_output_loss: 0.0116 - weight_output_loss: 0.0034 - bag_output_loss: 0.0052 - footwear_output_loss: 0.0052 - pose_output_loss: 0.0056 - emotion_output_loss: 0.0046 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9994 - age_output_acc: 0.9994 - weight_output_acc: 0.9997 - bag_output_acc: 0.9996 - footwear_output_acc: 0.9997 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9998 - val_loss: 4.2037 - val_gender_output_loss: 0.1929 - val_image_quality_output_loss: 0.4803 - val_age_output_loss: 0.8824 - val_weight_output_loss: 0.5170 - val_bag_output_loss: 0.4676 - val_footwear_output_loss: 0.5178 - val_pose_output_loss: 0.3051 - val_emotion_output_loss: 0.4360 - val_gender_output_acc: 0.9612 - val_image_quality_output_acc: 0.9052 - val_age_output_acc: 0.8548 - val_weight_output_acc: 0.9143 - val_bag_output_acc: 0.9113 - val_footwear_output_acc: 0.9113 - val_pose_output_acc: 0.9481 - val_emotion_output_acc: 0.9259\n",
            "Epoch 76/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4464 - gender_output_loss: 0.0014 - image_quality_output_loss: 0.0073 - age_output_loss: 0.0126 - weight_output_loss: 0.0027 - bag_output_loss: 0.0031 - footwear_output_loss: 0.0049 - pose_output_loss: 0.0054 - emotion_output_loss: 0.0048 - gender_output_acc: 0.9998 - image_quality_output_acc: 0.9992 - age_output_acc: 0.9988 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9997 - val_loss: 4.1285 - val_gender_output_loss: 0.1924 - val_image_quality_output_loss: 0.4727 - val_age_output_loss: 0.8845 - val_weight_output_loss: 0.4747 - val_bag_output_loss: 0.4617 - val_footwear_output_loss: 0.5191 - val_pose_output_loss: 0.2887 - val_emotion_output_loss: 0.4306 - val_gender_output_acc: 0.9612 - val_image_quality_output_acc: 0.9088 - val_age_output_acc: 0.8377 - val_weight_output_acc: 0.9219 - val_bag_output_acc: 0.9113 - val_footwear_output_acc: 0.9118 - val_pose_output_acc: 0.9456 - val_emotion_output_acc: 0.9269\n",
            "Epoch 77/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 219s 609ms/step - loss: 0.4382 - gender_output_loss: 7.9550e-04 - image_quality_output_loss: 0.0043 - age_output_loss: 0.0108 - weight_output_loss: 0.0021 - bag_output_loss: 0.0036 - footwear_output_loss: 0.0042 - pose_output_loss: 0.0052 - emotion_output_loss: 0.0034 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9995 - age_output_acc: 0.9994 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9998 - val_loss: 4.0789 - val_gender_output_loss: 0.1792 - val_image_quality_output_loss: 0.4732 - val_age_output_loss: 0.8737 - val_weight_output_loss: 0.4673 - val_bag_output_loss: 0.4380 - val_footwear_output_loss: 0.5211 - val_pose_output_loss: 0.2955 - val_emotion_output_loss: 0.4274 - val_gender_output_acc: 0.9617 - val_image_quality_output_acc: 0.9057 - val_age_output_acc: 0.8498 - val_weight_output_acc: 0.9204 - val_bag_output_acc: 0.9214 - val_footwear_output_acc: 0.9158 - val_pose_output_acc: 0.9531 - val_emotion_output_acc: 0.9239\n",
            "Epoch 78/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 610ms/step - loss: 0.4329 - gender_output_loss: 7.8339e-04 - image_quality_output_loss: 0.0027 - age_output_loss: 0.0091 - weight_output_loss: 0.0016 - bag_output_loss: 0.0032 - footwear_output_loss: 0.0043 - pose_output_loss: 0.0049 - emotion_output_loss: 0.0030 - gender_output_acc: 0.9999 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9997 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.0836 - val_gender_output_loss: 0.1840 - val_image_quality_output_loss: 0.4580 - val_age_output_loss: 0.8626 - val_weight_output_loss: 0.4865 - val_bag_output_loss: 0.4459 - val_footwear_output_loss: 0.5197 - val_pose_output_loss: 0.2952 - val_emotion_output_loss: 0.4288 - val_gender_output_acc: 0.9637 - val_image_quality_output_acc: 0.9163 - val_age_output_acc: 0.8488 - val_weight_output_acc: 0.9229 - val_bag_output_acc: 0.9199 - val_footwear_output_acc: 0.9183 - val_pose_output_acc: 0.9511 - val_emotion_output_acc: 0.9294\n",
            "Epoch 79/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4299 - gender_output_loss: 5.8530e-04 - image_quality_output_loss: 0.0022 - age_output_loss: 0.0082 - weight_output_loss: 0.0015 - bag_output_loss: 0.0029 - footwear_output_loss: 0.0042 - pose_output_loss: 0.0048 - emotion_output_loss: 0.0030 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9997 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1777 - val_gender_output_loss: 0.1813 - val_image_quality_output_loss: 0.4728 - val_age_output_loss: 0.8896 - val_weight_output_loss: 0.5056 - val_bag_output_loss: 0.4504 - val_footwear_output_loss: 0.5337 - val_pose_output_loss: 0.2989 - val_emotion_output_loss: 0.4431 - val_gender_output_acc: 0.9622 - val_image_quality_output_acc: 0.9158 - val_age_output_acc: 0.8508 - val_weight_output_acc: 0.9183 - val_bag_output_acc: 0.9183 - val_footwear_output_acc: 0.9118 - val_pose_output_acc: 0.9501 - val_emotion_output_acc: 0.9254\n",
            "Epoch 80/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4279 - gender_output_loss: 4.7682e-04 - image_quality_output_loss: 0.0017 - age_output_loss: 0.0079 - weight_output_loss: 0.0013 - bag_output_loss: 0.0026 - footwear_output_loss: 0.0041 - pose_output_loss: 0.0048 - emotion_output_loss: 0.0030 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9998 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9998 - val_loss: 4.1628 - val_gender_output_loss: 0.1802 - val_image_quality_output_loss: 0.4891 - val_age_output_loss: 0.8792 - val_weight_output_loss: 0.4832 - val_bag_output_loss: 0.4453 - val_footwear_output_loss: 0.5369 - val_pose_output_loss: 0.2966 - val_emotion_output_loss: 0.4505 - val_gender_output_acc: 0.9642 - val_image_quality_output_acc: 0.9062 - val_age_output_acc: 0.8528 - val_weight_output_acc: 0.9214 - val_bag_output_acc: 0.9239 - val_footwear_output_acc: 0.9158 - val_pose_output_acc: 0.9521 - val_emotion_output_acc: 0.9249\n",
            "Epoch 81/200\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4278 - gender_output_loss: 4.6439e-04 - image_quality_output_loss: 0.0020 - age_output_loss: 0.0084 - weight_output_loss: 0.0014 - bag_output_loss: 0.0027 - footwear_output_loss: 0.0038 - pose_output_loss: 0.0048 - emotion_output_loss: 0.0028 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9994 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.2275 - val_gender_output_loss: 0.1832 - val_image_quality_output_loss: 0.4825 - val_age_output_loss: 0.9014 - val_weight_output_loss: 0.4881 - val_bag_output_loss: 0.4459 - val_footwear_output_loss: 0.5449 - val_pose_output_loss: 0.3006 - val_emotion_output_loss: 0.4797 - val_gender_output_acc: 0.9612 - val_image_quality_output_acc: 0.9068 - val_age_output_acc: 0.8458 - val_weight_output_acc: 0.9189 - val_bag_output_acc: 0.9209 - val_footwear_output_acc: 0.9153 - val_pose_output_acc: 0.9501 - val_emotion_output_acc: 0.9214\n",
            "Epoch 82/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 610ms/step - loss: 0.4266 - gender_output_loss: 4.0285e-04 - image_quality_output_loss: 0.0015 - age_output_loss: 0.0084 - weight_output_loss: 0.0014 - bag_output_loss: 0.0024 - footwear_output_loss: 0.0038 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0031 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9995 - weight_output_acc: 0.9999 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9997 - val_loss: 4.1729 - val_gender_output_loss: 0.1852 - val_image_quality_output_loss: 0.4686 - val_age_output_loss: 0.8988 - val_weight_output_loss: 0.4889 - val_bag_output_loss: 0.4518 - val_footwear_output_loss: 0.5302 - val_pose_output_loss: 0.2998 - val_emotion_output_loss: 0.4486 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9148 - val_age_output_acc: 0.8508 - val_weight_output_acc: 0.9219 - val_bag_output_acc: 0.9214 - val_footwear_output_acc: 0.9168 - val_pose_output_acc: 0.9516 - val_emotion_output_acc: 0.9279\n",
            "Epoch 83/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4260 - gender_output_loss: 4.1203e-04 - image_quality_output_loss: 0.0017 - age_output_loss: 0.0080 - weight_output_loss: 0.0012 - bag_output_loss: 0.0026 - footwear_output_loss: 0.0037 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0027 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9998 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1529 - val_gender_output_loss: 0.1847 - val_image_quality_output_loss: 0.4673 - val_age_output_loss: 0.8944 - val_weight_output_loss: 0.4858 - val_bag_output_loss: 0.4493 - val_footwear_output_loss: 0.5261 - val_pose_output_loss: 0.2984 - val_emotion_output_loss: 0.4459 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9158 - val_age_output_acc: 0.8528 - val_weight_output_acc: 0.9209 - val_bag_output_acc: 0.9229 - val_footwear_output_acc: 0.9163 - val_pose_output_acc: 0.9531 - val_emotion_output_acc: 0.9289\n",
            "Epoch 84/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4251 - gender_output_loss: 3.7357e-04 - image_quality_output_loss: 0.0014 - age_output_loss: 0.0077 - weight_output_loss: 0.0011 - bag_output_loss: 0.0024 - footwear_output_loss: 0.0037 - pose_output_loss: 0.0048 - emotion_output_loss: 0.0026 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9996 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1633 - val_gender_output_loss: 0.1853 - val_image_quality_output_loss: 0.4698 - val_age_output_loss: 0.8948 - val_weight_output_loss: 0.4879 - val_bag_output_loss: 0.4511 - val_footwear_output_loss: 0.5278 - val_pose_output_loss: 0.2995 - val_emotion_output_loss: 0.4462 - val_gender_output_acc: 0.9637 - val_image_quality_output_acc: 0.9173 - val_age_output_acc: 0.8553 - val_weight_output_acc: 0.9219 - val_bag_output_acc: 0.9219 - val_footwear_output_acc: 0.9173 - val_pose_output_acc: 0.9531 - val_emotion_output_acc: 0.9284\n",
            "Epoch 85/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4245 - gender_output_loss: 4.3227e-04 - image_quality_output_loss: 0.0012 - age_output_loss: 0.0077 - weight_output_loss: 0.0011 - bag_output_loss: 0.0023 - footwear_output_loss: 0.0036 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0026 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1608 - val_gender_output_loss: 0.1856 - val_image_quality_output_loss: 0.4682 - val_age_output_loss: 0.8958 - val_weight_output_loss: 0.4875 - val_bag_output_loss: 0.4499 - val_footwear_output_loss: 0.5282 - val_pose_output_loss: 0.2990 - val_emotion_output_loss: 0.4456 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9173 - val_age_output_acc: 0.8579 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9224 - val_footwear_output_acc: 0.9168 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9274\n",
            "Epoch 86/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4248 - gender_output_loss: 3.5064e-04 - image_quality_output_loss: 0.0013 - age_output_loss: 0.0079 - weight_output_loss: 0.0012 - bag_output_loss: 0.0024 - footwear_output_loss: 0.0036 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0026 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9996 - weight_output_acc: 0.9999 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1545 - val_gender_output_loss: 0.1852 - val_image_quality_output_loss: 0.4679 - val_age_output_loss: 0.8924 - val_weight_output_loss: 0.4852 - val_bag_output_loss: 0.4500 - val_footwear_output_loss: 0.5283 - val_pose_output_loss: 0.2983 - val_emotion_output_loss: 0.4463 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9189 - val_age_output_acc: 0.8553 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9254 - val_footwear_output_acc: 0.9173 - val_pose_output_acc: 0.9521 - val_emotion_output_acc: 0.9269\n",
            "Epoch 87/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4245 - gender_output_loss: 3.9575e-04 - image_quality_output_loss: 0.0014 - age_output_loss: 0.0075 - weight_output_loss: 0.0012 - bag_output_loss: 0.0023 - footwear_output_loss: 0.0037 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0026 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9997 - weight_output_acc: 0.9999 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1517 - val_gender_output_loss: 0.1861 - val_image_quality_output_loss: 0.4687 - val_age_output_loss: 0.8911 - val_weight_output_loss: 0.4846 - val_bag_output_loss: 0.4480 - val_footwear_output_loss: 0.5300 - val_pose_output_loss: 0.2974 - val_emotion_output_loss: 0.4451 - val_gender_output_acc: 0.9622 - val_image_quality_output_acc: 0.9173 - val_age_output_acc: 0.8553 - val_weight_output_acc: 0.9214 - val_bag_output_acc: 0.9239 - val_footwear_output_acc: 0.9178 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9284\n",
            "Epoch 88/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4238 - gender_output_loss: 3.3789e-04 - image_quality_output_loss: 0.0011 - age_output_loss: 0.0076 - weight_output_loss: 9.7391e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0037 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0025 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9996 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1560 - val_gender_output_loss: 0.1854 - val_image_quality_output_loss: 0.4690 - val_age_output_loss: 0.8908 - val_weight_output_loss: 0.4872 - val_bag_output_loss: 0.4505 - val_footwear_output_loss: 0.5297 - val_pose_output_loss: 0.2982 - val_emotion_output_loss: 0.4444 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9189 - val_age_output_acc: 0.8558 - val_weight_output_acc: 0.9209 - val_bag_output_acc: 0.9224 - val_footwear_output_acc: 0.9199 - val_pose_output_acc: 0.9521 - val_emotion_output_acc: 0.9279\n",
            "Epoch 89/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4236 - gender_output_loss: 3.3438e-04 - image_quality_output_loss: 0.0012 - age_output_loss: 0.0074 - weight_output_loss: 9.8277e-04 - bag_output_loss: 0.0023 - footwear_output_loss: 0.0036 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0025 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1530 - val_gender_output_loss: 0.1851 - val_image_quality_output_loss: 0.4700 - val_age_output_loss: 0.8909 - val_weight_output_loss: 0.4857 - val_bag_output_loss: 0.4495 - val_footwear_output_loss: 0.5298 - val_pose_output_loss: 0.2981 - val_emotion_output_loss: 0.4431 - val_gender_output_acc: 0.9622 - val_image_quality_output_acc: 0.9178 - val_age_output_acc: 0.8553 - val_weight_output_acc: 0.9204 - val_bag_output_acc: 0.9249 - val_footwear_output_acc: 0.9194 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9289\n",
            "Epoch 90/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4238 - gender_output_loss: 3.8657e-04 - image_quality_output_loss: 0.0014 - age_output_loss: 0.0073 - weight_output_loss: 9.5630e-04 - bag_output_loss: 0.0023 - footwear_output_loss: 0.0036 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0024 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1615 - val_gender_output_loss: 0.1855 - val_image_quality_output_loss: 0.4718 - val_age_output_loss: 0.8906 - val_weight_output_loss: 0.4880 - val_bag_output_loss: 0.4502 - val_footwear_output_loss: 0.5319 - val_pose_output_loss: 0.2984 - val_emotion_output_loss: 0.4446 - val_gender_output_acc: 0.9632 - val_image_quality_output_acc: 0.9153 - val_age_output_acc: 0.8533 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9234 - val_footwear_output_acc: 0.9183 - val_pose_output_acc: 0.9521 - val_emotion_output_acc: 0.9274\n",
            "Epoch 91/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4229 - gender_output_loss: 3.8738e-04 - image_quality_output_loss: 0.0011 - age_output_loss: 0.0072 - weight_output_loss: 9.1941e-04 - bag_output_loss: 0.0023 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0023 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1569 - val_gender_output_loss: 0.1844 - val_image_quality_output_loss: 0.4705 - val_age_output_loss: 0.8899 - val_weight_output_loss: 0.4882 - val_bag_output_loss: 0.4476 - val_footwear_output_loss: 0.5300 - val_pose_output_loss: 0.2988 - val_emotion_output_loss: 0.4470 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9168 - val_age_output_acc: 0.8564 - val_weight_output_acc: 0.9199 - val_bag_output_acc: 0.9249 - val_footwear_output_acc: 0.9194 - val_pose_output_acc: 0.9541 - val_emotion_output_acc: 0.9274\n",
            "Epoch 92/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4233 - gender_output_loss: 3.2835e-04 - image_quality_output_loss: 0.0011 - age_output_loss: 0.0074 - weight_output_loss: 9.3727e-04 - bag_output_loss: 0.0024 - footwear_output_loss: 0.0036 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0025 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1572 - val_gender_output_loss: 0.1854 - val_image_quality_output_loss: 0.4677 - val_age_output_loss: 0.8922 - val_weight_output_loss: 0.4869 - val_bag_output_loss: 0.4508 - val_footwear_output_loss: 0.5293 - val_pose_output_loss: 0.2993 - val_emotion_output_loss: 0.4451 - val_gender_output_acc: 0.9622 - val_image_quality_output_acc: 0.9173 - val_age_output_acc: 0.8538 - val_weight_output_acc: 0.9209 - val_bag_output_acc: 0.9234 - val_footwear_output_acc: 0.9189 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9294\n",
            "Epoch 93/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 610ms/step - loss: 0.4242 - gender_output_loss: 4.1331e-04 - image_quality_output_loss: 0.0015 - age_output_loss: 0.0075 - weight_output_loss: 0.0010 - bag_output_loss: 0.0025 - footwear_output_loss: 0.0036 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0025 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9998 - age_output_acc: 0.9996 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1628 - val_gender_output_loss: 0.1845 - val_image_quality_output_loss: 0.4755 - val_age_output_loss: 0.8907 - val_weight_output_loss: 0.4867 - val_bag_output_loss: 0.4458 - val_footwear_output_loss: 0.5332 - val_pose_output_loss: 0.2987 - val_emotion_output_loss: 0.4473 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9158 - val_age_output_acc: 0.8558 - val_weight_output_acc: 0.9183 - val_bag_output_acc: 0.9244 - val_footwear_output_acc: 0.9183 - val_pose_output_acc: 0.9516 - val_emotion_output_acc: 0.9269\n",
            "Epoch 94/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4227 - gender_output_loss: 3.6720e-04 - image_quality_output_loss: 9.7878e-04 - age_output_loss: 0.0073 - weight_output_loss: 9.1175e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0024 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9996 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1541 - val_gender_output_loss: 0.1860 - val_image_quality_output_loss: 0.4681 - val_age_output_loss: 0.8885 - val_weight_output_loss: 0.4892 - val_bag_output_loss: 0.4490 - val_footwear_output_loss: 0.5293 - val_pose_output_loss: 0.2986 - val_emotion_output_loss: 0.4450 - val_gender_output_acc: 0.9632 - val_image_quality_output_acc: 0.9178 - val_age_output_acc: 0.8533 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9244 - val_footwear_output_acc: 0.9178 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9289\n",
            "Epoch 95/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4226 - gender_output_loss: 3.1249e-04 - image_quality_output_loss: 9.8794e-04 - age_output_loss: 0.0072 - weight_output_loss: 9.0167e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0036 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0024 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1618 - val_gender_output_loss: 0.1874 - val_image_quality_output_loss: 0.4689 - val_age_output_loss: 0.8907 - val_weight_output_loss: 0.4884 - val_bag_output_loss: 0.4506 - val_footwear_output_loss: 0.5312 - val_pose_output_loss: 0.2990 - val_emotion_output_loss: 0.4454 - val_gender_output_acc: 0.9632 - val_image_quality_output_acc: 0.9168 - val_age_output_acc: 0.8543 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9239 - val_footwear_output_acc: 0.9199 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9299\n",
            "Epoch 96/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4232 - gender_output_loss: 3.2364e-04 - image_quality_output_loss: 0.0011 - age_output_loss: 0.0074 - weight_output_loss: 0.0011 - bag_output_loss: 0.0023 - footwear_output_loss: 0.0037 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0024 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9996 - weight_output_acc: 0.9999 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9997 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1572 - val_gender_output_loss: 0.1867 - val_image_quality_output_loss: 0.4694 - val_age_output_loss: 0.8916 - val_weight_output_loss: 0.4853 - val_bag_output_loss: 0.4490 - val_footwear_output_loss: 0.5301 - val_pose_output_loss: 0.3001 - val_emotion_output_loss: 0.4448 - val_gender_output_acc: 0.9632 - val_image_quality_output_acc: 0.9183 - val_age_output_acc: 0.8558 - val_weight_output_acc: 0.9199 - val_bag_output_acc: 0.9249 - val_footwear_output_acc: 0.9204 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9304\n",
            "Epoch 97/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4224 - gender_output_loss: 3.4765e-04 - image_quality_output_loss: 0.0011 - age_output_loss: 0.0071 - weight_output_loss: 9.0495e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0024 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1597 - val_gender_output_loss: 0.1884 - val_image_quality_output_loss: 0.4688 - val_age_output_loss: 0.8922 - val_weight_output_loss: 0.4863 - val_bag_output_loss: 0.4493 - val_footwear_output_loss: 0.5311 - val_pose_output_loss: 0.2990 - val_emotion_output_loss: 0.4445 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9178 - val_age_output_acc: 0.8543 - val_weight_output_acc: 0.9199 - val_bag_output_acc: 0.9249 - val_footwear_output_acc: 0.9189 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9284\n",
            "Epoch 98/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4225 - gender_output_loss: 3.2132e-04 - image_quality_output_loss: 0.0010 - age_output_loss: 0.0072 - weight_output_loss: 9.5345e-04 - bag_output_loss: 0.0023 - footwear_output_loss: 0.0036 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0024 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1633 - val_gender_output_loss: 0.1890 - val_image_quality_output_loss: 0.4720 - val_age_output_loss: 0.8916 - val_weight_output_loss: 0.4870 - val_bag_output_loss: 0.4498 - val_footwear_output_loss: 0.5307 - val_pose_output_loss: 0.2989 - val_emotion_output_loss: 0.4442 - val_gender_output_acc: 0.9632 - val_image_quality_output_acc: 0.9178 - val_age_output_acc: 0.8543 - val_weight_output_acc: 0.9199 - val_bag_output_acc: 0.9254 - val_footwear_output_acc: 0.9199 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9284\n",
            "Epoch 99/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4224 - gender_output_loss: 3.1970e-04 - image_quality_output_loss: 0.0011 - age_output_loss: 0.0072 - weight_output_loss: 8.9615e-04 - bag_output_loss: 0.0023 - footwear_output_loss: 0.0036 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0023 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9998 - val_loss: 4.1664 - val_gender_output_loss: 0.1886 - val_image_quality_output_loss: 0.4719 - val_age_output_loss: 0.8923 - val_weight_output_loss: 0.4878 - val_bag_output_loss: 0.4518 - val_footwear_output_loss: 0.5297 - val_pose_output_loss: 0.2990 - val_emotion_output_loss: 0.4453 - val_gender_output_acc: 0.9632 - val_image_quality_output_acc: 0.9183 - val_age_output_acc: 0.8538 - val_weight_output_acc: 0.9199 - val_bag_output_acc: 0.9239 - val_footwear_output_acc: 0.9199 - val_pose_output_acc: 0.9536 - val_emotion_output_acc: 0.9284\n",
            "Epoch 100/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4222 - gender_output_loss: 2.9545e-04 - image_quality_output_loss: 0.0011 - age_output_loss: 0.0072 - weight_output_loss: 9.0060e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0023 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1664 - val_gender_output_loss: 0.1894 - val_image_quality_output_loss: 0.4703 - val_age_output_loss: 0.8928 - val_weight_output_loss: 0.4859 - val_bag_output_loss: 0.4521 - val_footwear_output_loss: 0.5310 - val_pose_output_loss: 0.2999 - val_emotion_output_loss: 0.4451 - val_gender_output_acc: 0.9632 - val_image_quality_output_acc: 0.9183 - val_age_output_acc: 0.8553 - val_weight_output_acc: 0.9183 - val_bag_output_acc: 0.9234 - val_footwear_output_acc: 0.9189 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9289\n",
            "Epoch 101/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 610ms/step - loss: 0.4220 - gender_output_loss: 3.2494e-04 - image_quality_output_loss: 9.7336e-04 - age_output_loss: 0.0071 - weight_output_loss: 9.0208e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0023 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1722 - val_gender_output_loss: 0.1893 - val_image_quality_output_loss: 0.4721 - val_age_output_loss: 0.8945 - val_weight_output_loss: 0.4875 - val_bag_output_loss: 0.4509 - val_footwear_output_loss: 0.5320 - val_pose_output_loss: 0.2993 - val_emotion_output_loss: 0.4467 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9168 - val_age_output_acc: 0.8574 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9234 - val_footwear_output_acc: 0.9189 - val_pose_output_acc: 0.9536 - val_emotion_output_acc: 0.9294\n",
            "Epoch 102/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4222 - gender_output_loss: 3.3861e-04 - image_quality_output_loss: 0.0012 - age_output_loss: 0.0072 - weight_output_loss: 8.6166e-04 - bag_output_loss: 0.0023 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0023 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1630 - val_gender_output_loss: 0.1888 - val_image_quality_output_loss: 0.4704 - val_age_output_loss: 0.8918 - val_weight_output_loss: 0.4874 - val_bag_output_loss: 0.4496 - val_footwear_output_loss: 0.5305 - val_pose_output_loss: 0.2991 - val_emotion_output_loss: 0.4456 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9173 - val_age_output_acc: 0.8574 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9244 - val_footwear_output_acc: 0.9204 - val_pose_output_acc: 0.9531 - val_emotion_output_acc: 0.9289\n",
            "Epoch 103/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4220 - gender_output_loss: 3.2152e-04 - image_quality_output_loss: 9.7827e-04 - age_output_loss: 0.0071 - weight_output_loss: 8.7619e-04 - bag_output_loss: 0.0023 - footwear_output_loss: 0.0036 - pose_output_loss: 0.0047 - emotion_output_loss: 0.0024 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1730 - val_gender_output_loss: 0.1894 - val_image_quality_output_loss: 0.4748 - val_age_output_loss: 0.8937 - val_weight_output_loss: 0.4879 - val_bag_output_loss: 0.4493 - val_footwear_output_loss: 0.5330 - val_pose_output_loss: 0.2984 - val_emotion_output_loss: 0.4465 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9158 - val_age_output_acc: 0.8569 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9234 - val_footwear_output_acc: 0.9194 - val_pose_output_acc: 0.9531 - val_emotion_output_acc: 0.9294\n",
            "Epoch 104/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4214 - gender_output_loss: 2.9641e-04 - image_quality_output_loss: 9.4211e-04 - age_output_loss: 0.0070 - weight_output_loss: 8.8283e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0022 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1779 - val_gender_output_loss: 0.1894 - val_image_quality_output_loss: 0.4742 - val_age_output_loss: 0.8948 - val_weight_output_loss: 0.4914 - val_bag_output_loss: 0.4495 - val_footwear_output_loss: 0.5325 - val_pose_output_loss: 0.2998 - val_emotion_output_loss: 0.4465 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9168 - val_age_output_acc: 0.8574 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9239 - val_footwear_output_acc: 0.9214 - val_pose_output_acc: 0.9536 - val_emotion_output_acc: 0.9289\n",
            "Epoch 105/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4215 - gender_output_loss: 3.2330e-04 - image_quality_output_loss: 0.0010 - age_output_loss: 0.0071 - weight_output_loss: 8.1063e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0034 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0022 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1787 - val_gender_output_loss: 0.1896 - val_image_quality_output_loss: 0.4741 - val_age_output_loss: 0.8958 - val_weight_output_loss: 0.4898 - val_bag_output_loss: 0.4511 - val_footwear_output_loss: 0.5325 - val_pose_output_loss: 0.2994 - val_emotion_output_loss: 0.4468 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9173 - val_age_output_acc: 0.8564 - val_weight_output_acc: 0.9209 - val_bag_output_acc: 0.9229 - val_footwear_output_acc: 0.9204 - val_pose_output_acc: 0.9536 - val_emotion_output_acc: 0.9299\n",
            "Epoch 106/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4217 - gender_output_loss: 2.9885e-04 - image_quality_output_loss: 9.6496e-04 - age_output_loss: 0.0071 - weight_output_loss: 9.2577e-04 - bag_output_loss: 0.0021 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0024 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9998 - val_loss: 4.1771 - val_gender_output_loss: 0.1898 - val_image_quality_output_loss: 0.4729 - val_age_output_loss: 0.8968 - val_weight_output_loss: 0.4912 - val_bag_output_loss: 0.4517 - val_footwear_output_loss: 0.5312 - val_pose_output_loss: 0.2995 - val_emotion_output_loss: 0.4443 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9178 - val_age_output_acc: 0.8569 - val_weight_output_acc: 0.9214 - val_bag_output_acc: 0.9249 - val_footwear_output_acc: 0.9194 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9299\n",
            "Epoch 107/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4211 - gender_output_loss: 2.9406e-04 - image_quality_output_loss: 8.7764e-04 - age_output_loss: 0.0070 - weight_output_loss: 8.6641e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0034 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0022 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1716 - val_gender_output_loss: 0.1890 - val_image_quality_output_loss: 0.4733 - val_age_output_loss: 0.8949 - val_weight_output_loss: 0.4884 - val_bag_output_loss: 0.4492 - val_footwear_output_loss: 0.5324 - val_pose_output_loss: 0.2989 - val_emotion_output_loss: 0.4457 - val_gender_output_acc: 0.9622 - val_image_quality_output_acc: 0.9178 - val_age_output_acc: 0.8579 - val_weight_output_acc: 0.9214 - val_bag_output_acc: 0.9244 - val_footwear_output_acc: 0.9194 - val_pose_output_acc: 0.9536 - val_emotion_output_acc: 0.9284\n",
            "Epoch 108/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4219 - gender_output_loss: 3.1929e-04 - image_quality_output_loss: 0.0011 - age_output_loss: 0.0073 - weight_output_loss: 9.1913e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0024 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9995 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9998 - val_loss: 4.1732 - val_gender_output_loss: 0.1904 - val_image_quality_output_loss: 0.4719 - val_age_output_loss: 0.8936 - val_weight_output_loss: 0.4880 - val_bag_output_loss: 0.4520 - val_footwear_output_loss: 0.5334 - val_pose_output_loss: 0.2994 - val_emotion_output_loss: 0.4450 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9183 - val_age_output_acc: 0.8548 - val_weight_output_acc: 0.9214 - val_bag_output_acc: 0.9234 - val_footwear_output_acc: 0.9178 - val_pose_output_acc: 0.9531 - val_emotion_output_acc: 0.9279\n",
            "Epoch 109/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4212 - gender_output_loss: 2.9765e-04 - image_quality_output_loss: 0.0011 - age_output_loss: 0.0070 - weight_output_loss: 8.0761e-04 - bag_output_loss: 0.0021 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0023 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9999 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9997 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1824 - val_gender_output_loss: 0.1910 - val_image_quality_output_loss: 0.4737 - val_age_output_loss: 0.8945 - val_weight_output_loss: 0.4884 - val_bag_output_loss: 0.4524 - val_footwear_output_loss: 0.5361 - val_pose_output_loss: 0.3006 - val_emotion_output_loss: 0.4463 - val_gender_output_acc: 0.9632 - val_image_quality_output_acc: 0.9163 - val_age_output_acc: 0.8553 - val_weight_output_acc: 0.9199 - val_bag_output_acc: 0.9244 - val_footwear_output_acc: 0.9194 - val_pose_output_acc: 0.9531 - val_emotion_output_acc: 0.9304\n",
            "Epoch 110/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4214 - gender_output_loss: 3.0790e-04 - image_quality_output_loss: 0.0012 - age_output_loss: 0.0071 - weight_output_loss: 8.2295e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0022 - gender_output_acc: 1.0000 - image_quality_output_acc: 0.9998 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9998 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1860 - val_gender_output_loss: 0.1890 - val_image_quality_output_loss: 0.4739 - val_age_output_loss: 0.8955 - val_weight_output_loss: 0.4902 - val_bag_output_loss: 0.4515 - val_footwear_output_loss: 0.5376 - val_pose_output_loss: 0.3010 - val_emotion_output_loss: 0.4479 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9163 - val_age_output_acc: 0.8553 - val_weight_output_acc: 0.9204 - val_bag_output_acc: 0.9234 - val_footwear_output_acc: 0.9194 - val_pose_output_acc: 0.9521 - val_emotion_output_acc: 0.9304\n",
            "Epoch 111/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 221s 613ms/step - loss: 0.4210 - gender_output_loss: 2.8895e-04 - image_quality_output_loss: 9.2233e-04 - age_output_loss: 0.0071 - weight_output_loss: 8.1311e-04 - bag_output_loss: 0.0021 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0022 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9996 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1774 - val_gender_output_loss: 0.1898 - val_image_quality_output_loss: 0.4706 - val_age_output_loss: 0.8955 - val_weight_output_loss: 0.4895 - val_bag_output_loss: 0.4514 - val_footwear_output_loss: 0.5339 - val_pose_output_loss: 0.3002 - val_emotion_output_loss: 0.4472 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9163 - val_age_output_acc: 0.8543 - val_weight_output_acc: 0.9209 - val_bag_output_acc: 0.9239 - val_footwear_output_acc: 0.9204 - val_pose_output_acc: 0.9521 - val_emotion_output_acc: 0.9289\n",
            "Epoch 112/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 0.4207 - gender_output_loss: 2.6801e-04 - image_quality_output_loss: 8.7448e-04 - age_output_loss: 0.0070 - weight_output_loss: 7.9474e-04 - bag_output_loss: 0.0021 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0046 - emotion_output_loss: 0.0022 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1801 - val_gender_output_loss: 0.1895 - val_image_quality_output_loss: 0.4716 - val_age_output_loss: 0.8938 - val_weight_output_loss: 0.4901 - val_bag_output_loss: 0.4533 - val_footwear_output_loss: 0.5345 - val_pose_output_loss: 0.3001 - val_emotion_output_loss: 0.4479 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9153 - val_age_output_acc: 0.8533 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9239 - val_footwear_output_acc: 0.9204 - val_pose_output_acc: 0.9531 - val_emotion_output_acc: 0.9289\n",
            "Epoch 113/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 221s 613ms/step - loss: 0.4192 - gender_output_loss: 2.6740e-04 - image_quality_output_loss: 8.9694e-04 - age_output_loss: 0.0070 - weight_output_loss: 8.0878e-04 - bag_output_loss: 0.0022 - footwear_output_loss: 0.0034 - pose_output_loss: 0.0032 - emotion_output_loss: 0.0022 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9998 - emotion_output_acc: 0.9999 - val_loss: 4.1946 - val_gender_output_loss: 0.1909 - val_image_quality_output_loss: 0.4717 - val_age_output_loss: 0.8975 - val_weight_output_loss: 0.4922 - val_bag_output_loss: 0.4564 - val_footwear_output_loss: 0.5347 - val_pose_output_loss: 0.3024 - val_emotion_output_loss: 0.4494 - val_gender_output_acc: 0.9632 - val_image_quality_output_acc: 0.9173 - val_age_output_acc: 0.8553 - val_weight_output_acc: 0.9204 - val_bag_output_acc: 0.9239 - val_footwear_output_acc: 0.9204 - val_pose_output_acc: 0.9531 - val_emotion_output_acc: 0.9299\n",
            "Epoch 114/200\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 0.4203 - gender_output_loss: 2.5660e-04 - image_quality_output_loss: 8.4238e-04 - age_output_loss: 0.0069 - weight_output_loss: 7.9932e-04 - bag_output_loss: 0.0021 - footwear_output_loss: 0.0035 - pose_output_loss: 0.0045 - emotion_output_loss: 0.0022 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9997 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9998 - pose_output_acc: 0.9997 - emotion_output_acc: 0.9999 - val_loss: 4.1940 - val_gender_output_loss: 0.1913 - val_image_quality_output_loss: 0.4728 - val_age_output_loss: 0.8973 - val_weight_output_loss: 0.4923 - val_bag_output_loss: 0.4552 - val_footwear_output_loss: 0.5353 - val_pose_output_loss: 0.3020 - val_emotion_output_loss: 0.4486 - val_gender_output_acc: 0.9627 - val_image_quality_output_acc: 0.9153 - val_age_output_acc: 0.8538 - val_weight_output_acc: 0.9194 - val_bag_output_acc: 0.9234 - val_footwear_output_acc: 0.9189 - val_pose_output_acc: 0.9526 - val_emotion_output_acc: 0.9294\n",
            "Epoch 115/200\n",
            "Learning rate:  0.0001\n",
            "245/360 [===================>..........] - ETA: 1:06 - loss: 0.4258 - gender_output_loss: 2.9213e-04 - image_quality_output_loss: 9.2698e-04 - age_output_loss: 0.0096 - weight_output_loss: 7.9840e-04 - bag_output_loss: 0.0028 - footwear_output_loss: 0.0027 - pose_output_loss: 0.0065 - emotion_output_loss: 0.0029 - gender_output_acc: 1.0000 - image_quality_output_acc: 1.0000 - age_output_acc: 0.9995 - weight_output_acc: 1.0000 - bag_output_acc: 0.9999 - footwear_output_acc: 0.9999 - pose_output_acc: 0.9996 - emotion_output_acc: 0.9999"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-617:\n",
            "Process ForkPoolWorker-615:\n",
            "Process ForkPoolWorker-618:\n",
            "Process ForkPoolWorker-622:\n",
            "Process ForkPoolWorker-619:\n",
            "Process ForkPoolWorker-613:\n",
            "Process ForkPoolWorker-614:\n",
            "Process ForkPoolWorker-621:\n",
            "Process ForkPoolWorker-624:\n",
            "Process ForkPoolWorker-623:\n",
            "Process ForkPoolWorker-620:\n",
            "Process ForkPoolWorker-616:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-aa3ca11be710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waswhiJkZHqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('gdrive/My Drive/Assignment5/model1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79DSxquMA6Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Run training, with or without data augmentation.\n",
        "# if not data_augmentation:\n",
        "#     print('Not using data augmentation.')\n",
        "#     model.fit(x_train, y_train,\n",
        "#               batch_size=batch_size,\n",
        "#               epochs=epochs,\n",
        "#               validation_data=(x_test, y_test),\n",
        "#               shuffle=True,\n",
        "#               callbacks=callbacks)\n",
        "# else:\n",
        "#     print('Using real-time data augmentation.')\n",
        "#     # This will do preprocessing and realtime data augmentation:\n",
        "#     datagen = ImageDataGenerator(\n",
        "#         # set input mean to 0 over the dataset\n",
        "#         featurewise_center=False,\n",
        "#         # set each sample mean to 0\n",
        "#         samplewise_center=False,\n",
        "#         # divide inputs by std of dataset\n",
        "#         featurewise_std_normalization=False,\n",
        "#         # divide each input by its std\n",
        "#         samplewise_std_normalization=False,\n",
        "#         # apply ZCA whitening\n",
        "#         zca_whitening=False,\n",
        "#         # epsilon for ZCA whitening\n",
        "#         zca_epsilon=1e-06,\n",
        "#         # randomly rotate images in the range (deg 0 to 180)\n",
        "#         rotation_range=0,\n",
        "#         # randomly shift images horizontally\n",
        "#         width_shift_range=0.1,\n",
        "#         # randomly shift images vertically\n",
        "#         height_shift_range=0.1,\n",
        "#         # set range for random shear\n",
        "#         shear_range=0.,\n",
        "#         # set range for random zoom\n",
        "#         zoom_range=0.,\n",
        "#         # set range for random channel shifts\n",
        "#         channel_shift_range=0.,\n",
        "#         # set mode for filling points outside the input boundaries\n",
        "#         fill_mode='nearest',\n",
        "#         # value used for fill_mode = \"constant\"\n",
        "#         cval=0.,\n",
        "#         # randomly flip images\n",
        "#         horizontal_flip=True,\n",
        "#         # randomly flip images\n",
        "#         vertical_flip=False,\n",
        "#         # set rescaling factor (applied before any other transformation)\n",
        "#         rescale=None,\n",
        "#         # set function that will be applied on each input\n",
        "#         preprocessing_function=None,\n",
        "#         # image data format, either \"channels_first\" or \"channels_last\"\n",
        "#         data_format=None,\n",
        "#         # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "#         validation_split=0.0)\n",
        "\n",
        "#     # Compute quantities required for featurewise normalization\n",
        "#     # (std, mean, and principal components if ZCA whitening is applied).\n",
        "#     datagen.fit(x_train)\n",
        "\n",
        "#     # Fit the model on the batches generated by datagen.flow().\n",
        "#     model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "#                         validation_data=(x_test, y_test),\n",
        "#                         epochs=epochs, verbose=1, workers=4,\n",
        "#                         callbacks=callbacks)\n",
        "\n",
        "# # Score trained model.\n",
        "# scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "# print('Test loss:', scores[0])\n",
        "# print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
