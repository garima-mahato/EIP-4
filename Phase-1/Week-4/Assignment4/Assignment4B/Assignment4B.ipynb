{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4B.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3FCCaakn_uw",
        "colab_type": "code",
        "outputId": "fdcf62cc-d120-4808-a992-32cd0716b80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "## Connecting google drive to colab\n",
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 135718 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.14-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.14-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.14-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cbRzE50oGeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Cg4bz6W39C",
        "colab_type": "code",
        "outputId": "8685bb8c-ff3e-446f-9829-7c548ab9daba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import cv2\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import savefig\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdVZI7BdnQpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW642dnpIVn0",
        "colab_type": "text"
      },
      "source": [
        "## ResNet Model Creation and Training\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE4b1PLCLlEe",
        "colab_type": "code",
        "outputId": "9ff978f2-7eba-45c2-e47e-50de99c4e1e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 50 #200\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "pixel_level = False\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 64\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs,num_filters=num_filters)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "\n",
        "    x = Conv2D(num_classes,(1,1),padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(1e-4))(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    #x = AveragePooling2D(pool_size=8)(x)\n",
        "    #y = Flatten()(x)\n",
        "    # outputs = Dense(num_classes,\n",
        "    #                 activation='softmax',\n",
        "    #                 kernel_initializer='he_normal')(y)\n",
        "\n",
        "    outputs = Activation('softmax')(x)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-V_gtmzWSGo",
        "colab_type": "code",
        "outputId": "431eddec-682e-44ac-c5b3-fbddb1c75703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 64)   1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 64)   36928       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 64)   36928       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           activation_1[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 64)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   36928       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           activation_3[0][0]               \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 64)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   36928       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 64)   0           activation_5[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 64)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 128)  73856       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 128)  512         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 128)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 128)  147584      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  8320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 128)  0           conv2d_10[0][0]                  \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 128)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 128)  147584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 128)  0           activation_9[0][0]               \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 128)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 128)  147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 128)  147584      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 16, 16, 128)  0           activation_11[0][0]              \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 128)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 256)    295168      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 256)    1024        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 256)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 256)    590080      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 256)    33024       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 256)    1024        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 256)    0           conv2d_17[0][0]                  \n",
            "                                                                 batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 256)    0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 256)    590080      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 256)    1024        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 256)    590080      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 256)    1024        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 256)    0           activation_15[0][0]              \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 256)    0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 256)    590080      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 256)    1024        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 256)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 256)    590080      activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 256)    1024        conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 8, 8, 256)    0           activation_17[0][0]              \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 256)    0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 10)     2570        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 10)           0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 10)           0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 4,335,626\n",
            "Trainable params: 4,330,122\n",
            "Non-trainable params: 5,504\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04VaIbeGFxOK",
        "colab_type": "code",
        "outputId": "1d1244d0-e9e7-4c63-b147-ef1cc58faad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "dirname = 'drive/EIP 4/Session4'\n",
        "save_dir = os.path.join(dirname, 'saved_models')\n",
        "model_name = 'cifar10_%s_model.ep{epoch:03d}.val{val_acc}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=pixel_level),\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 32, 32, 64)   1792        input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 32, 32, 64)   256         conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 32, 32, 64)   0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 32, 32, 64)   36928       activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 32, 32, 64)   256         conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 32, 32, 64)   0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 32, 32, 64)   36928       activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 32, 32, 64)   256         conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_70 (Add)                    (None, 32, 32, 64)   0           activation_155[0][0]             \n",
            "                                                                 batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 32, 32, 64)   0           add_70[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 32, 32, 64)   36928       activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 32, 32, 64)   256         conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 32, 32, 64)   0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 32, 32, 64)   36928       activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 32, 32, 64)   256         conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_71 (Add)                    (None, 32, 32, 64)   0           activation_157[0][0]             \n",
            "                                                                 batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 32, 32, 64)   0           add_71[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 32, 32, 64)   36928       activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 32, 32, 64)   256         conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 32, 32, 64)   0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 32, 32, 64)   36928       activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 32, 32, 64)   256         conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_72 (Add)                    (None, 32, 32, 64)   0           activation_159[0][0]             \n",
            "                                                                 batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 32, 32, 64)   0           add_72[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 16, 16, 128)  73856       activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 16, 16, 128)  512         conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 16, 16, 128)  0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 16, 16, 128)  147584      activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 16, 16, 128)  8320        activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 16, 16, 128)  512         conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_73 (Add)                    (None, 16, 16, 128)  0           conv2d_183[0][0]                 \n",
            "                                                                 batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 16, 16, 128)  0           add_73[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 16, 16, 128)  147584      activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 16, 16, 128)  512         conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 16, 16, 128)  0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 16, 16, 128)  147584      activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 16, 16, 128)  512         conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_74 (Add)                    (None, 16, 16, 128)  0           activation_163[0][0]             \n",
            "                                                                 batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 16, 16, 128)  0           add_74[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 16, 16, 128)  147584      activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 16, 16, 128)  512         conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 16, 16, 128)  0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 16, 16, 128)  147584      activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 16, 16, 128)  512         conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_75 (Add)                    (None, 16, 16, 128)  0           activation_165[0][0]             \n",
            "                                                                 batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 16, 16, 128)  0           add_75[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 8, 8, 256)    295168      activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 8, 8, 256)    1024        conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 8, 8, 256)    0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 8, 8, 256)    590080      activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 8, 8, 256)    33024       activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 8, 8, 256)    1024        conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_76 (Add)                    (None, 8, 8, 256)    0           conv2d_190[0][0]                 \n",
            "                                                                 batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 8, 8, 256)    0           add_76[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 8, 8, 256)    590080      activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 8, 8, 256)    1024        conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 8, 8, 256)    0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 8, 8, 256)    590080      activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 8, 8, 256)    1024        conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_77 (Add)                    (None, 8, 8, 256)    0           activation_169[0][0]             \n",
            "                                                                 batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 8, 8, 256)    0           add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 8, 8, 256)    590080      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 8, 8, 256)    1024        conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 8, 8, 256)    0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 8, 8, 256)    590080      activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 8, 8, 256)    1024        conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_78 (Add)                    (None, 8, 8, 256)    0           activation_171[0][0]             \n",
            "                                                                 batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 8, 8, 256)    0           add_78[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 8, 8, 10)     2570        activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_5 (Glo (None, 10)           0           conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 10)           0           global_average_pooling2d_5[0][0] \n",
            "==================================================================================================\n",
            "Total params: 4,335,626\n",
            "Trainable params: 4,330,122\n",
            "Non-trainable params: 5,504\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 123s 79ms/step - loss: 1.9886 - acc: 0.4543 - val_loss: 1.7893 - val_acc: 0.5166\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.51660, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep001.val0.5166.h5\n",
            "Epoch 2/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 1.4076 - acc: 0.6233 - val_loss: 2.0636 - val_acc: 0.5055\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.51660\n",
            "Epoch 3/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 1.2265 - acc: 0.6880 - val_loss: 1.3721 - val_acc: 0.6409\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.51660 to 0.64090, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep003.val0.6409.h5\n",
            "Epoch 4/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 1.1244 - acc: 0.7282 - val_loss: 1.2641 - val_acc: 0.6809\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.64090 to 0.68090, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep004.val0.6809.h5\n",
            "Epoch 5/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 1.0608 - acc: 0.7486 - val_loss: 1.1157 - val_acc: 0.7423\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.68090 to 0.74230, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep005.val0.7423.h5\n",
            "Epoch 6/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 1.0107 - acc: 0.7682 - val_loss: 1.1429 - val_acc: 0.7298\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.74230\n",
            "Epoch 7/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.9716 - acc: 0.7788 - val_loss: 1.4637 - val_acc: 0.6480\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.74230\n",
            "Epoch 8/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.9338 - acc: 0.7909 - val_loss: 1.0316 - val_acc: 0.7580\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.74230 to 0.75800, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep008.val0.758.h5\n",
            "Epoch 9/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.9042 - acc: 0.7992 - val_loss: 0.9162 - val_acc: 0.8041\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.75800 to 0.80410, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep009.val0.8041.h5\n",
            "Epoch 10/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.8750 - acc: 0.8092 - val_loss: 1.0229 - val_acc: 0.7663\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.80410\n",
            "Epoch 11/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.8541 - acc: 0.8145 - val_loss: 1.5772 - val_acc: 0.6576\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.80410\n",
            "Epoch 12/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.8320 - acc: 0.8187 - val_loss: 1.0894 - val_acc: 0.7562\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.80410\n",
            "Epoch 13/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.8144 - acc: 0.8255 - val_loss: 0.9833 - val_acc: 0.7743\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.80410\n",
            "Epoch 14/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.7977 - acc: 0.8262 - val_loss: 0.8055 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.80410 to 0.83600, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep014.val0.836.h5\n",
            "Epoch 15/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.7793 - acc: 0.8321 - val_loss: 0.7857 - val_acc: 0.8314\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.83600\n",
            "Epoch 16/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 69ms/step - loss: 0.7694 - acc: 0.8335 - val_loss: 0.9464 - val_acc: 0.8029\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.83600\n",
            "Epoch 17/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 69ms/step - loss: 0.7593 - acc: 0.8367 - val_loss: 0.7550 - val_acc: 0.8456\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.83600 to 0.84560, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep017.val0.8456.h5\n",
            "Epoch 18/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.7498 - acc: 0.8407 - val_loss: 1.0467 - val_acc: 0.7753\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.84560\n",
            "Epoch 19/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.7390 - acc: 0.8433 - val_loss: 0.8318 - val_acc: 0.8266\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.84560\n",
            "Epoch 20/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 69ms/step - loss: 0.7259 - acc: 0.8455 - val_loss: 0.8364 - val_acc: 0.8233\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.84560\n",
            "Epoch 21/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.7228 - acc: 0.8458 - val_loss: 0.7566 - val_acc: 0.8335\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.84560\n",
            "Epoch 22/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.7103 - acc: 0.8515 - val_loss: 1.0281 - val_acc: 0.7720\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.84560\n",
            "Epoch 23/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.7062 - acc: 0.8511 - val_loss: 0.7398 - val_acc: 0.8403\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.84560\n",
            "Epoch 24/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 113s 72ms/step - loss: 0.7026 - acc: 0.8504 - val_loss: 0.8160 - val_acc: 0.8233\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.84560\n",
            "Epoch 25/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.6952 - acc: 0.8530 - val_loss: 0.6959 - val_acc: 0.8529\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.84560 to 0.85290, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep025.val0.8529.h5\n",
            "Epoch 26/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 109s 70ms/step - loss: 0.6865 - acc: 0.8556 - val_loss: 0.6677 - val_acc: 0.8693\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.85290 to 0.86930, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep026.val0.8693.h5\n",
            "Epoch 27/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 112s 72ms/step - loss: 0.6814 - acc: 0.8577 - val_loss: 0.6889 - val_acc: 0.8538\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.86930\n",
            "Epoch 28/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6759 - acc: 0.8578 - val_loss: 0.7214 - val_acc: 0.8514\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.86930\n",
            "Epoch 29/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 111s 71ms/step - loss: 0.6728 - acc: 0.8606 - val_loss: 0.7669 - val_acc: 0.8347\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.86930\n",
            "Epoch 30/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 111s 71ms/step - loss: 0.6721 - acc: 0.8581 - val_loss: 0.8302 - val_acc: 0.8169\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.86930\n",
            "Epoch 31/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 111s 71ms/step - loss: 0.6623 - acc: 0.8609 - val_loss: 0.7882 - val_acc: 0.8384\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.86930\n",
            "Epoch 32/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 111s 71ms/step - loss: 0.6625 - acc: 0.8609 - val_loss: 0.7384 - val_acc: 0.8478\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.86930\n",
            "Epoch 33/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 71ms/step - loss: 0.6553 - acc: 0.8638 - val_loss: 0.9764 - val_acc: 0.7966\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.86930\n",
            "Epoch 34/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 71ms/step - loss: 0.6550 - acc: 0.8639 - val_loss: 0.7461 - val_acc: 0.8470\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.86930\n",
            "Epoch 35/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 111s 71ms/step - loss: 0.6549 - acc: 0.8623 - val_loss: 0.7323 - val_acc: 0.8511\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.86930\n",
            "Epoch 36/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 71ms/step - loss: 0.6524 - acc: 0.8629 - val_loss: 0.7037 - val_acc: 0.8540\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.86930\n",
            "Epoch 37/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6479 - acc: 0.8666 - val_loss: 0.7179 - val_acc: 0.8462\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.86930\n",
            "Epoch 38/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6471 - acc: 0.8660 - val_loss: 0.9743 - val_acc: 0.7924\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.86930\n",
            "Epoch 39/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6428 - acc: 0.8684 - val_loss: 0.8423 - val_acc: 0.8182\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.86930\n",
            "Epoch 40/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6432 - acc: 0.8671 - val_loss: 0.6562 - val_acc: 0.8675\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.86930\n",
            "Epoch 41/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6417 - acc: 0.8671 - val_loss: 0.9237 - val_acc: 0.7920\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.86930\n",
            "Epoch 42/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6412 - acc: 0.8657 - val_loss: 0.6837 - val_acc: 0.8623\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.86930\n",
            "Epoch 43/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 71ms/step - loss: 0.6319 - acc: 0.8696 - val_loss: 0.6700 - val_acc: 0.8643\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.86930\n",
            "Epoch 44/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 71ms/step - loss: 0.6261 - acc: 0.8703 - val_loss: 0.7115 - val_acc: 0.8530\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.86930\n",
            "Epoch 45/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 71ms/step - loss: 0.6274 - acc: 0.8695 - val_loss: 0.7142 - val_acc: 0.8557\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.86930\n",
            "Epoch 46/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6304 - acc: 0.8689 - val_loss: 0.7716 - val_acc: 0.8297\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.86930\n",
            "Epoch 47/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 71ms/step - loss: 0.6258 - acc: 0.8718 - val_loss: 0.7171 - val_acc: 0.8477\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.86930\n",
            "Epoch 48/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6233 - acc: 0.8700 - val_loss: 0.7586 - val_acc: 0.8387\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.86930\n",
            "Epoch 49/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6220 - acc: 0.8709 - val_loss: 0.7419 - val_acc: 0.8402\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.86930\n",
            "Epoch 50/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 110s 70ms/step - loss: 0.6239 - acc: 0.8705 - val_loss: 0.6473 - val_acc: 0.8714\n",
            "\n",
            "Epoch 00050: val_acc improved from 0.86930 to 0.87140, saving model to drive/EIP 4/Session4/saved_models/cifar10_ResNet20v1_model.ep050.val0.8714.h5\n",
            "10000/10000 [==============================] - 6s 619us/step\n",
            "Test loss: 0.6472819014549256\n",
            "Test accuracy: 0.8714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oelO2iybdQIH",
        "colab_type": "text"
      },
      "source": [
        "## Loading Model Weights and Predicting on Custom Images\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhiPRl-WK2Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(\"drive/EIP 4/Session4/saved_models/model87.14PercentValAcc/cifar10_ResNet20v1_model.ep050.val0.8714.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxKphQSfQmHB",
        "colab_type": "code",
        "outputId": "cbf33eeb-afb5-4a14-c49f-858437c026cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_images = 10\n",
        "width, height = 32, 32\n",
        "dataset = []\n",
        "\n",
        "for i in range(1, num_images+1):\n",
        "    img = cv2.imread(\"drive/EIP 4/Session4/images/img\" +str(i) +\".jpg\")\n",
        "\n",
        "    dim = (width, height)\n",
        "    # resize image\n",
        "    resized = cv2.resize(img, dim)\n",
        "\n",
        "    dataset.append(np.array(resized))\n",
        "\n",
        "dataset_source = np.asarray(dataset)\n",
        "print(dataset_source.shape)\n",
        "\n",
        "\n",
        "# Normalize data.\n",
        "x_custom_test = dataset_source.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_custom_test -= x_train_mean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbG2XSieU6gX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(x_custom_test)\n",
        "\n",
        "class_idx = np.argmax(predictions, axis=1) #axis=1 for row wise max\n",
        "class_idx = np.reshape(class_idx, (len(class_idx),1))\n",
        "\n",
        "y_test_idx = class_idx\n",
        "\n",
        "last_conv_layer = model.get_layer(\"activation_19\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pds83HjPZIl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Labels of class index:\n",
        "\n",
        "airplane : 0\n",
        "automobile : 1\n",
        "bird : 2\n",
        "cat : 3\n",
        "deer : 4\n",
        "dog : 5\n",
        "frog : 6\n",
        "horse : 7\n",
        "ship : 8\n",
        "truck : 9\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "CLASS_LABELS = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpdcjACPWcgh",
        "colab_type": "code",
        "outputId": "ff520b9e-4b42-41fc-d970-97280e3b93bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        }
      },
      "source": [
        "fig=plt.figure(figsize=(15, 40))\n",
        "#fig.subplots_adjust(wspace = 0.9)\n",
        "columns = 5\n",
        "rows = 10\n",
        "ind = 1\n",
        "\n",
        "for index in range(num_images):\n",
        "  #index = misclassified_images_index[mis_index]\n",
        "  class_output = model.output[:, int(y_test_idx[index][0])]\n",
        "  \n",
        "  grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
        "  pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "  iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "  pooled_grads_value, conv_layer_output_value = iterate([np.expand_dims(x_test[index], axis=0)])\n",
        "  for i in range(256):\n",
        "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "  \n",
        "  heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "  heatmap = np.maximum(heatmap, 0)\n",
        "  heatmap /= np.max(heatmap)\n",
        "  \n",
        "  img = x_custom_test[index]\n",
        "  heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "  heatmap = np.uint8(255 * heatmap)\n",
        "  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "  #print(heatmap.shape)\n",
        "  superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.2, 0, dtype = cv2.CV_32F)\n",
        "  \n",
        "  a = fig.add_subplot(rows, columns, ind)\n",
        "  \n",
        "  plt.imshow(img)\n",
        "  plt.imshow(superimposed_img)\n",
        "  \n",
        "  a.set_title(f\"{index+1}) Predicted Class: {CLASS_LABELS[class_idx[index][0]]}\")\n",
        "  \n",
        "  ind = ind + 1\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3EAAAGcCAYAAAB3MSk5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niSOEEEIIIYSQHPH/AVXOOP/yBl65AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x2880 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
